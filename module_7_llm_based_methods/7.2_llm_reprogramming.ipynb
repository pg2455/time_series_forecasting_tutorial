{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-based Approaches for Forecasting: Reprogramming GPT-2\n",
    "\n",
    "**Approximate Learning Time**:Up to 3 hours\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will learn about the recent work by [Jin et al. (2023)](https://arxiv.org/abs/2310.01728) on reprogramming an open-source LLM such as GPT-2 or LLaMA for time series forecasting. We will be using `neuralforecast` library to train TimeLLM model on our dataset.\n",
    "\n",
    "--- \n",
    "## TimeLLM\n",
    "\n",
    "We have already learned about LLMs in the previous notebook. Some LLMs are closed-source, meaning that their model weights and training data are not accessible, which limits their use to direct interaction. On the other hand, there are smaller LLMs, like GPT-2 or the LLaMA series, that are open-source, meaning both their model weights and architecture are accessible to everyone.\n",
    "\n",
    "In their work, the authors leverage pre-trained models like GPT-2 or LLaMA by:\n",
    "\n",
    "1. **Initializing the prompt**: The prompt is initialized with a basic description of the time series, including its characteristics and some statistics. This prompt serves as a prefix for the input.\n",
    "\n",
    "2. **Processing the time series**: The time series is normalized and divided into patches using a sliding 1D window, where each window creates a patch of length $ L $. These patches are embedded into a dimension $ D $. The token embeddings of the LLM are also reduced to a smaller subspace using linear probing, transforming them from $ \\mathcal{R}^{V \\times D} $ to $ \\mathcal{R}^{V' \\times D} $, where $ V' \\ll V $; the authors call these prototypes. Self-attention is then applied to map the query (time series patches) to the values (prototypes embeddings). The idea is to learn a relationship between the text and time series modalities, thereby enabling a cross-domain adaptations. These patch embeddings are then appended to the prompt embedding from the previous step.\n",
    "\n",
    "3. **Generating predictions**: Once the LLM is prompted with the concatenated embeddings, the output embeddings are passed through a neural network tasked with predicting the next steps in the time series. \n",
    "\n",
    "\n",
    "The network then computes the loss and gradients are backpropagated. During backpropagation, gradients are computed throughout the model, including in the LLM. However, the LLM itself is kept frozen, meaning its weights are not updated. This approach allows the authors to train only a lightweight neural network on top of the LLM, leading to accurate time series forecasts.\n",
    "\n",
    "Figure below is from the paper and depicts the above process visually: \n",
    "\n",
    "<div style=\"text-align: center; padding: 20px;\">\n",
    "<img src=\"images/timellm.png\" style=\"max-width: 70%; clip-path: inset(2px); height: auto; border-radius: 15px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Reading [paper](https://arxiv.org/abs/2310.01728) and going through the author's [GitHub repo](https://github.com/KimMeen/Time-LLM) are a good resource to understand further. \n",
    "\n",
    "\n",
    "**References**:\n",
    "\n",
    "[(Jin et al. 2023) Time-LLM: Time Series Forecasting by Reprogramming Large Language Models](https://arxiv.org/abs/2310.01728)\n",
    "\n",
    "---\n",
    "\n",
    "Let's load the log daily returns of exchange rates, and split the data into train, validation, and test subsets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import TimeLLM\n",
    "from transformers import GPT2Config, GPT2Model, GPT2Tokenizer\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## WARNING: To compare different models on the same horizon, keep this same across the notebooks\n",
    "from termcolor import colored\n",
    "import sys; sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "FORECASTING_HORIZON = [4, 8, 12, 24, 52] # weeks \n",
    "MAX_FORECASTING_HORIZON = max(FORECASTING_HORIZON)\n",
    "\n",
    "SEQUENCE_LENGTH = 2 * MAX_FORECASTING_HORIZON\n",
    "PREDICTION_LENGTH = MAX_FORECASTING_HORIZON\n",
    "\n",
    "DIRECTORY_PATH_TO_SAVE_RESULTS = pathlib.Path('../results/DIY/').resolve()\n",
    "MODEL_NAME = \"TimeLLM\"\n",
    "\n",
    "RESULTS_DIRECTORY = DIRECTORY_PATH_TO_SAVE_RESULTS / MODEL_NAME\n",
    "if RESULTS_DIRECTORY.exists():\n",
    "    print(colored(f'Directory {str(RESULTS_DIRECTORY)} already exists.'\n",
    "           '\\nThis notebook will overwrite results in the same directory.'\n",
    "           '\\nYou can also create a new directory if you want to keep this directory untouched.'\n",
    "           ' Just change the `MODEL_NAME` in this notebook.\\n', \"red\" ))\n",
    "else:\n",
    "    RESULTS_DIRECTORY.mkdir(parents=True)\n",
    "\n",
    "data, transformed_data = utils.load_tutotrial_data(dataset='exchange_rate', log_transform=True)\n",
    "data = transformed_data \n",
    "\n",
    "train_val_data = data.iloc[:-MAX_FORECASTING_HORIZON]\n",
    "train_data, val_data = train_val_data.iloc[:-MAX_FORECASTING_HORIZON], train_val_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "test_data = data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "print(f\"Number of steps in training data: {len(train_data)}\\nNumber of steps in validation data: {len(val_data)}\\nNumber of steps in test data: {len(test_data)}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Data Handling for Neuralforecast\n",
    "\n",
    "The data format requirements can be found [here](https://nixtlaverse.nixtla.io/neuralforecast/docs/tutorials/getting_started_complete.html). The format requires three specific keys:\n",
    "- `unique_id`\n",
    "- `ds`\n",
    "- `y`\n",
    "\n",
    "Therefore, we will convert our dataframe into the format required by `neuralforecast` to proceed with the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data\n",
    "\n",
    "nf_train_val_data = pd.melt(train_val_data.reset_index(), id_vars=['date'], value_vars=train_val_data.columns, value_name='y')\n",
    "\n",
    "nf_train_val_data = nf_train_val_data.rename(columns={\n",
    "        'date': 'ds',\n",
    "        'variable': 'unique_id'\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## TimeLLM via Neuralforeacast \n",
    "\n",
    "In **this tutorial**, we will use TimeLLM's implementation from the `neuralforecast` library, which provides an interface similar to Lightning framework. While it is not the most polished option, as the library is still under development, it offers a convenient starting point. As of September 2024, there aren't many other libraries available for TimeLLM, so itâ€™s worthwhile to check if a more updated version of `neuralforecast` is available or explore alternative libraries that might offer better support for TimeLLM.\n",
    "\n",
    "Implementation of the model in `neuralforecast` can be found [here](https://github.com/Nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb).\n",
    "\n",
    "\n",
    "The model loading might take around 10-15 minutes depending on the processor speed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the open-sourced LLM that we will use \n",
    "gpt2_config = GPT2Config.from_pretrained('openai-community/gpt2')\n",
    "gpt2 = GPT2Model.from_pretrained('openai-community/gpt2', config=gpt2_config)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "# define the model\n",
    "prompt_prefix = \"The dataset contains data on exchange rate across 8 countries.\"\n",
    "\n",
    "timellm = TimeLLM(\n",
    "            h=MAX_FORECASTING_HORIZON,\n",
    "            input_size=SEQUENCE_LENGTH,\n",
    "            llm=gpt2,\n",
    "            llm_config=gpt2_config,\n",
    "            llm_tokenizer=gpt2_tokenizer,\n",
    "            prompt_prefix=prompt_prefix,\n",
    "            batch_size=64,\n",
    "            valid_batch_size=4,\n",
    "            max_steps=1000, # will determine the time to train \n",
    "        )\n",
    "\n",
    "# let nf know what are the models\n",
    "nf = NeuralForecast(\n",
    "    models=[timellm],\n",
    "    freq=data.index.freq,\n",
    ")\n",
    "\n",
    "# fit it on data\n",
    "nf.fit(df=nf_train_val_data, val_size=MAX_FORECASTING_HORIZON, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = nf.predict(df=nf_train_val_data)\n",
    "\n",
    "# \n",
    "df = forecasts.reset_index()\n",
    "df = df.pivot(columns=['unique_id'], index='ds')\n",
    "df.columns = df.columns.droplevel(0)\n",
    "df.index.name = 'date'\n",
    "\n",
    "test_predictions_df = df\n",
    "AUGMENTED_COL_NAMES = [f\"{MODEL_NAME}_{col}_mean\" for col in data.columns]\n",
    "test_predictions_df.columns = AUGMENTED_COL_NAMES\n",
    "\n",
    "# ssave them to the directory\n",
    "test_predictions_df.to_csv(f\"{str(RESULTS_DIRECTORY)}/predictions.csv\", index=True)\n",
    "print(test_predictions_df.shape)\n",
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Evaluate \n",
    "\n",
    "Let's compute the metrics by comparing the predictions with that of the target data. Note that we will have to rename the columns of the dataframe to match the expected column names by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalaute metrics\n",
    "target_data = data[-MAX_FORECASTING_HORIZON:]\n",
    "model_metrics, records = utils.get_mase_metrics(\n",
    "    historical_data=train_val_data,\n",
    "    test_predictions=test_predictions_df.rename(\n",
    "            columns={x:x.split(\"_\")[1] for x in test_predictions_df.columns\n",
    "        }),\n",
    "    target_data=target_data,\n",
    "    forecasting_horizons=FORECASTING_HORIZON,\n",
    "    columns=data.columns, \n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "records.to_csv(f\"{str(RESULTS_DIRECTORY)}/metrics.csv\", index=False)\n",
    "records[['col', 'horizon', 'mase']].pivot(index=['horizon'], columns='col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_results(path=DIRECTORY_PATH_TO_SAVE_RESULTS, metric='mase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Plot Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = utils.plot_forecasts(\n",
    "    historical_data=train_val_data,\n",
    "    forecast_directory_path=DIRECTORY_PATH_TO_SAVE_RESULTS,\n",
    "    target_data=target_data,\n",
    "    columns=data.columns,\n",
    "    n_history_to_plot=10, \n",
    "    forecasting_horizon=MAX_FORECASTING_HORIZON,\n",
    "    dpi=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We learned about TimeLLM, an approach to reprogram open-sourced LLMs for time series forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "- Use LLaMA instead of GPT-2 for the model.\n",
    "\n",
    "- Select and optimize hyperparameters for the model, such as the embedding dimension and others, to achieve the best performance.\n",
    "\n",
    "- Apply a normalization procedure (e.g., **min-max scaling**) to the data, ensuring that only the training data is used for fitting the scaler. Perform the modeling process on the normalized data and, after generating the final model's predictions, invert the normalization to return the output to its original scale. See `sklearn.preprocessing.MinMaxScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))\n",
    "\n",
    "- Additionally, perform the modeling on the **raw data**, without applying any transformation (such as converting it into log daily returns), to compare results directly with the untransformed dataset.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Kudos! You've made it to the end of the tutorial. I hope you're now feeling more confident navigating the complex landscape of forecasting methods. Best of luck in your future forecasting endeavors!\n",
    "\n",
    "If you have a moment, I'd really appreciate your feedback. You can share your thoughts using the survey link provided in `0_introduction.ipynb`. Thank you!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chains1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
