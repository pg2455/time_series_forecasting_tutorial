{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31b75a0-3c13-4513-9d5b-46cc4d4fc4aa",
   "metadata": {},
   "source": [
    "# Time Series Datasets Using GluonTS Library\n",
    "\n",
    "**Approximate Learning time:** Up to 1 hour\n",
    "\n",
    "--- \n",
    "\n",
    "In this notebook, we will explore a second method for accessing time series datasets. The GluonTS library (documentation) offers several functionalities for handling time series data, specifically for modeling purposes. The library also provides an interface to a wide variety of models. Although still under active development, the research community frequently uses its data-handling capabilities when building models.\n",
    "\n",
    "We will use GluonTS for data handling in Module 6, so this notebook serves as a good introduction to interfacing with GluonTS, with the primary focus on exploring the datasets available within the library.\n",
    "\n",
    "---\n",
    "\n",
    "## Datasets through GluonTS\n",
    "\n",
    "GluonTS provides the `gluonts.dataset` package ([documentation](https://ts.gluon.ai/stable/api/gluonts/gluonts.dataset.html)), which includes a subpackage `gluonts.dataset.repository` for downloading commonly available datasets. You can view the list of available datasets in the [source code](https://ts.gluon.ai/stable/_modules/gluonts/dataset/repository/datasets.html#get_download_path) or by accessing the `dataset_names` variable in the subpackage. Many of these datasets are similar to those we have explored in previous notebooks.\n",
    "\n",
    "**Note:** The first download may take some time as the dataset is stored in `$HOME/.gluonts`. Subsequent accesses will check this directory before downloading again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76a46cd-4975-41d2-920d-26714eaec8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets available in GluonTS 62\n",
      "Available datasets:  ['constant', 'exchange_rate', 'solar-energy', 'electricity', 'traffic', 'exchange_rate_nips', 'electricity_nips', 'traffic_nips', 'solar_nips', 'wiki2000_nips', 'wiki-rolling_nips', 'taxi_30min', 'kaggle_web_traffic_with_missing', 'kaggle_web_traffic_without_missing', 'kaggle_web_traffic_weekly', 'm1_yearly', 'm1_quarterly', 'm1_monthly', 'nn5_daily_with_missing', 'nn5_daily_without_missing', 'nn5_weekly', 'tourism_monthly', 'tourism_quarterly', 'tourism_yearly', 'cif_2016', 'london_smart_meters_without_missing', 'wind_farms_without_missing', 'car_parts_without_missing', 'dominick', 'fred_md', 'pedestrian_counts', 'hospital', 'covid_deaths', 'kdd_cup_2018_without_missing', 'weather', 'm3_monthly', 'm3_quarterly', 'm3_yearly', 'm3_other', 'm4_hourly', 'm4_daily', 'm4_weekly', 'm4_monthly', 'm4_quarterly', 'm4_yearly', 'm5', 'uber_tlc_daily', 'uber_tlc_hourly', 'airpassengers', 'australian_electricity_demand', 'electricity_hourly', 'electricity_weekly', 'rideshare_without_missing', 'saugeenday', 'solar_10_minutes', 'solar_weekly', 'sunspot_without_missing', 'temperature_rain_without_missing', 'vehicle_trips_without_missing', 'ercot', 'ett_small_15min', 'ett_small_1h']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gupta/Workspace/tutorials/time_series_v3/forecasting/lib/python3.11/site-packages/gluonts/json.py:102: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gluonts.dataset.repository.datasets import get_dataset, dataset_names\n",
    "\n",
    "print(\"Number of datasets available in GluonTS\", len(dataset_names))\n",
    "print(\"Available datasets: \", dataset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1f265-0b95-4aa8-836f-cdef88ebbb75",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Traffic Dataset\n",
    "\n",
    "The **Traffic dataset** is a multivariate dataset containing 48 months (2015-2016) of hourly data from the California Department of Transportation. It includes road occupancy rates (values between 0 and 1) measured by various sensors on San Francisco Bay area freeways. This dataset was first introduced in Lai et al. (2017). \n",
    "\n",
    "\n",
    "[(Lai et al. 2017) Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks](https://arxiv.org/abs/1703.07015)\n",
    "\n",
    "--- \n",
    "\n",
    "GluonTS loads datasets using the function `get_datasets`([docs](https://ts.gluon.ai/stable/api/gluonts/gluonts.dataset.repository.html#gluonts.dataset.repository.get_dataset)). The loaded datasets are of type `TrainDatasets` ([docs](https://ts.gluon.ai/stable/api/gluonts/gluonts.dataset.common.html#gluonts.dataset.common.TrainDatasets)), which contains three main attributes: metadata, train, and test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad39782e-8c0e-43ab-8570-ff4ac2d6112b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaData(freq='H', target=None, feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='862')], feat_static_real=[], feat_dynamic_real=[], feat_dynamic_cat=[], prediction_length=24)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_dataset(\"traffic\")\n",
    "dataset.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524002f",
   "metadata": {},
   "source": [
    "**Metadata**:\n",
    "GluonTS datasets include metadata that provides specific information about the time series, including:\n",
    "\n",
    "- **Frequency of Data**: Indicates the time interval of the data points (e.g., hourly 'H', monthly 'M').\n",
    "- **Prediction Length**: Specifies the length of the forecasting horizon.\n",
    "- **Features**: A dataset can contain four types of features:\n",
    "    - **feat_static_cat**: These are static categorical features. The ID of a time series is specified by this feature, and its cardinality indicates the number of time series. Other such features, representing different aspects of the time series, can also be present and are labeled as `feat_static_cat_x`.\n",
    "    - **feat_static_real**: These are static real-valued features that remain constant for a particular time series.\n",
    "    - **feat_dynamic_cat**: These categorical features are dynamic, meaning they change over time. However, their values at each time step $t$ are available before the target (or prediction) is observed, making them useful as features in modeling.\n",
    "    - **feat_dynamic_real**: Similarly, these are real-valued features that vary over time.\n",
    "\n",
    "- **target**: This is a list of the time series to be predicted. If none is specified, all unique categories in `feat_static_cat_0` are used as targets. This also influences how the training and testing split is handled, which we will cover in subsequent modules.\n",
    "\n",
    "**Train & Test**:\n",
    "\n",
    "There are various methods to split a time series dataset into training and testing subsets, which we will explore in detail starting in Module 3. \n",
    "In general, for time series forecasting tasks, the training dataset consists of values up to a certain time step. The test dataset includes all of those values plus additional time steps that need to be forecasted.\n",
    "\n",
    "For example, GluonTS organizes its datasets with the minimum of the following attributes:\n",
    "- **'target'**: Each time series is represented as a 1D numpy array.\n",
    "- **'start'**: Contains information about the starting point and the frequency of the observations.\n",
    "- **'item_id'**: The unique identifier for each time series.\n",
    "\n",
    "The test dataset will have the same attributes as the training dataset but with more values corresponding to the additional time steps that need to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d0bf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': array([0.0048, 0.0072, 0.004 , ..., 0.053 , 0.0533, 0.05  ], dtype=float32),\n",
       " 'start': Period('2015-01-01 00:00', 'H'),\n",
       " 'feat_static_cat': array([0], dtype=int32),\n",
       " 'item_id': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b508153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in train: 14036\n",
      "Number of observations in corresponding test: 14060\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of observations in train: {next(iter(dataset.train))['target'].shape[0]}\")\n",
    "print(f\"Number of observations in corresponding test: {next(iter(dataset.test))['target'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62f0ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Transformation with GluonTS\n",
    "\n",
    "GluonTS provides a wide range of templated features to transform time series data. \n",
    "To take advantage of this, we need to define transformation templates and let GluonTS apply them to the datasets. \n",
    "\n",
    "In this section, we will define one such transformation.\n",
    "\n",
    "**Transformation Steps:**\n",
    "\n",
    "1. **Remove Unwanted Features**: In `dataset.train`, the feature 'feat_static_cat' is present. We will remove this feature.\n",
    "2. **Ensure Data Format**: Verify that the data is in the form of a NumPy array.\n",
    "3. **Add Time Features**: We will add time-related features that correspond to the time index, such as the month, week of the year, and others. These features are important at the point where predictions are made, and GluonTS easily handles this. Additionally, you can use `time_features_from_frequency_str`, which recommends the appropriate time features based on the frequency of the dataset.\n",
    "4. **Add an Age Feature**: Although not commonly used in traditional models, the age feature becomes relevant in transformer-like architectures. We will learn more about these models in **Module 6**.\n",
    "5. **Stack Time and Age Features**: Once the time and age features are created, we will stack them together for further processing.\n",
    "6. **Rename Keys**: Finally, we will rename certain internal keys: change `time_feat` to `time_features` and `target` to `values` for clarity.\n",
    "\n",
    "While most of the transformation functions are self-explanatory, some common arguments they take include: `target_field` denoting the time series, `output_field`, specifying the name of the field where the transformed dataset will be stored, and `start_field` indicating the starting datetime index.\n",
    "\n",
    "\n",
    "Applying these transformations is straightforward: define them in a `Chain` and pass the dataset through the constructed transformation. Setting `is_train=True` ensures that the returned dataset has the same length as the target field already present in the dataset. If `is_train=False`, it will return `len(target) + prediction_length` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd0833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended time features:  [<function hour_of_day at 0x125f171a0>, <function day_of_week at 0x125f172e0>, <function day_of_month at 0x125f17420>, <function day_of_year at 0x125f17560>]\n",
      "{'start': Period('2015-01-01 00:00', 'H'), 'item_id': 0, 'time_features': array([[-0.5       , -0.45652175, -0.41304347, ...,  0.23913044,\n",
      "         0.2826087 ,  0.32608697],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.5       ,\n",
      "         0.5       ,  0.5       ],\n",
      "       [-0.5       , -0.5       , -0.5       , ..., -0.3       ,\n",
      "        -0.3       , -0.3       ],\n",
      "       [-0.5       , -0.5       , -0.5       , ...,  0.1       ,\n",
      "         0.1       ,  0.1       ],\n",
      "       [ 0.30103   ,  0.47712126,  0.60206   , ...,  4.1472125 ,\n",
      "         4.1472435 ,  4.1472745 ]], dtype=float32), 'values': array([0.0048, 0.0072, 0.004 , ..., 0.053 , 0.0533, 0.05  ], dtype=float32)}\n",
      "Shape of time features:  (5, 14036)\n",
      "Shape of values:  (14036,)\n"
     ]
    }
   ],
   "source": [
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "from gluonts.dataset.field_names import FieldName # Offers a mapping from attributes to string names \n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddTimeFeatures, \n",
    "    Chain,\n",
    "    RemoveFields,\n",
    "    RenameFields,\n",
    "    AsNumpyArray,\n",
    "    VstackFeatures,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Recommended time features: \", time_features_from_frequency_str(dataset.metadata.freq))\n",
    "\n",
    "remove_field_names=[FieldName.FEAT_STATIC_REAL, FieldName.FEAT_DYNAMIC_REAL, FieldName.FEAT_STATIC_CAT]\n",
    "transformation = Chain(\n",
    "    [RemoveFields(field_names=remove_field_names)]\n",
    "    + [\n",
    "        AsNumpyArray(\n",
    "            field=FieldName.TARGET,\n",
    "            expected_ndim=1,\n",
    "        ),\n",
    "        AddTimeFeatures(\n",
    "            start_field=FieldName.START,\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.FEAT_TIME,\n",
    "            time_features=time_features_from_frequency_str(dataset.metadata.freq),\n",
    "            pred_length=24,\n",
    "        ),\n",
    "        AddAgeFeature(\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.FEAT_AGE,\n",
    "            pred_length=24,\n",
    "            log_scale=True,\n",
    "        ),\n",
    "        VstackFeatures(\n",
    "            output_field=FieldName.FEAT_TIME,\n",
    "            input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
    "        ),\n",
    "        RenameFields(\n",
    "            mapping={\n",
    "                FieldName.FEAT_TIME: \"time_features\",\n",
    "                FieldName.TARGET: \"values\",\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for batch in transformation.apply(dataset.train, is_train=True):\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "print(\"Shape of time features: \", batch['time_features'].shape)\n",
    "print(\"Shape of values: \", batch['values'].shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b436dd6e-fc8d-4267-86b9-05bbc163cb49",
   "metadata": {},
   "source": [
    "--- \n",
    "## Conclusion\n",
    "\n",
    "We have learned how GluonTS handles time series datasets and explored its functionality for transforming them. This experience will be valuable when we build forecasting models using the Informer architecture in Module 6.\n",
    "\n",
    "---\n",
    "## Next Steps\n",
    "\n",
    "Now that we are familiar with various time series datasets and tools to explore them, let's formally define the problem of forecasting in the next module. We will learn about dataset splitting required for modeling and various evaluation metrics to assess the modeling performance. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
