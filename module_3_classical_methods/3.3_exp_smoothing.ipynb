{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bce813-4c66-4678-a7d7-d865f7d3c859",
   "metadata": {},
   "source": [
    "# Classical Forecasting Methods: Holt-Winter's Model\n",
    "\n",
    "**Approximate Learning Time**: Up to 2 hours\n",
    "\n",
    "--- \n",
    "\n",
    "## Holt-Winters Model\n",
    "\n",
    "The Holt-Winters model is a method used for forecasting time series data that shows both **trend** and **seasonality**. It works by applying three smoothing techniques:\n",
    "\n",
    "1. **Level smoothing**: Captures the overall average of the series.\n",
    "2. **Trend smoothing**: Captures the direction and rate of change in the data over time.\n",
    "3. **Seasonal smoothing**: Captures repeating patterns (seasonality) in the data at regular intervals.\n",
    "\n",
    "By combining these three components, Holt-Winters can make predictions that account for both long-term trends and short-term seasonal fluctuations. In simple terms, itâ€™s like saying: \"The model predicts the future by looking at the average, the trend over time, and any repeating patterns in the data.\" There are several simplications of this model, which are all broadly termed as **Exponential Smoothing models** ([Wikipedia](https://en.wikipedia.org/wiki/Exponential_smoothing)). \n",
    "\n",
    "**Note**: These models are designed for univariate time series, meaning it models one time series at a time. Therefore, we will build individual univariate models for each of the time series in our dataset. To handle **multivariate time series**, where multiple variables are modeled together, we will introduce the **Vector Autoregression (VAR)** approach in the next notebook.\n",
    "\n",
    "--- \n",
    "\n",
    "Let's load the log daily returns of exchange rates, and split the data into train, validation, and test subsets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a476e-7956-4ae3-b5bd-17d82f8b91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "import sys; sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing\n",
    "\n",
    "# To avoid flooding of the screen with convergence warnings during hyperparameter tuning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## WARNING: To compare different models on the same horizon, keep this same across the notebooks \n",
    "FORECASTING_HORIZON = [4, 8, 12] # weeks \n",
    "MAX_FORECASTING_HORIZON = max(FORECASTING_HORIZON)\n",
    "\n",
    "SEQUENCE_LENGTH = 2 * MAX_FORECASTING_HORIZON\n",
    "PREDICTION_LENGTH = MAX_FORECASTING_HORIZON\n",
    "\n",
    "DIRECTORY_PATH_TO_SAVE_RESULTS = pathlib.Path('../results/DIY/').resolve()\n",
    "MODEL_NAME = \"ExpSmooth\"\n",
    "\n",
    "RESULTS_DIRECTORY = DIRECTORY_PATH_TO_SAVE_RESULTS / MODEL_NAME\n",
    "if RESULTS_DIRECTORY.exists():\n",
    "    print(colored(f'Directory {str(RESULTS_DIRECTORY)} already exists.'\n",
    "           '\\nThis notebook will overwrite results in the same directory.'\n",
    "           '\\nYou can also create a new directory if you want to keep this directory untouched.'\n",
    "           ' Just change the `MODEL_NAME` in this notebook.\\n', \"red\" ))\n",
    "else:\n",
    "    RESULTS_DIRECTORY.mkdir(parents=True)\n",
    "\n",
    "\n",
    "# load data\n",
    "\n",
    "data, transformed_data = utils.load_tutotrial_data(dataset='exchange_rate', log_transform=True)\n",
    "data = transformed_data\n",
    "\n",
    "train_val_data = data.iloc[:-MAX_FORECASTING_HORIZON]\n",
    "train_data, val_data = train_val_data.iloc[:-MAX_FORECASTING_HORIZON], train_val_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "test_data = data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "\n",
    "print(f\"Number of steps in training data: {len(train_data)}\\nNumber of steps in validation data: {len(val_data)}\\nNumber of steps in test data: {len(test_data)}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688ba21-561c-43ad-8570-f8a85efd8a57",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "These models are provided in `statsmodels` library. Specifically, they are provided through `SimpleExpSmoothing` ([documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html)) and a more general `ExponentialSmoothing` ([documentation](https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html)) functions. \n",
    "\n",
    "In this tutorial, we will run a hyperparameter search on the values of `seasonal_period`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc95c4d-5390-4338-bfc1-a13684a66fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_values = [0, 2, 4, 8, 12]\n",
    "best_mase, best_mase_model = {}, {}\n",
    "for col in train_data.columns:\n",
    "    best_mase[col] = np.inf\n",
    "    best_mase_model[col] = {}\n",
    "\n",
    "    for s in s_values:\n",
    "        if s <= 1: # Holt's Linear Trend Model\n",
    "            model = SimpleExpSmoothing(train_data[col], initialization_method=\"estimated\")\n",
    "        else: # Holt-Winter's Model\n",
    "            model = ExponentialSmoothing(train_data[col], trend='add', seasonal='add', seasonal_periods=s, initialization_method=\"estimated\")\n",
    "        \n",
    "        model = model.fit()\n",
    "        y_pred = model.forecast(len(val_data))\n",
    "        mase = utils.mean_absolute_scaled_error(y_pred.values, val_data[col].values, train_data[col].values)\n",
    "        \n",
    "        if mase < best_mase[col]:\n",
    "            best_mase[col] = mase\n",
    "            best_mase_model[col] = (model, model.aic, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39839d95-cb24-4431-8714-956a9f24c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best paramerters\")\n",
    "for col in train_data.columns:\n",
    "    mase = best_mase[col]\n",
    "    s = best_mase_model[col][2]\n",
    "    print(f\"Col: {col}\\tValidation-MASE:{mase: 0.3f}\\tSeasonality:{s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec949c-bf67-48b0-b8af-d234b2c7cc2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Refit on Train-Val Subset & Forecast\n",
    "\n",
    "To measure the model's performance on the test data, we will first retrain the model using the combined train-validation dataset. Then, we will compute the MASE metric on the test dataset to evaluate its performance.\n",
    "Additionally, we will store the test predictions for later comparison with other forecasting methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251e18d-ae7f-4a6f-ad77-4d518756f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = {}\n",
    "best_model_metrics = {}\n",
    "for col in test_data.columns:\n",
    "    best_model_metrics[col] = {}\n",
    "    ts = train_val_data[col]\n",
    "    \n",
    "    # retrain the model with best mase parameters on train_val_data\n",
    "    s = best_mase_model[col][2]\n",
    "    if s <= 1: # Holt's Linear Trend Model\n",
    "        model = SimpleExpSmoothing(ts, initialization_method=\"estimated\")\n",
    "    else: # Holt-Winter's Model\n",
    "        model = ExponentialSmoothing(ts, trend='add',  seasonal='add', seasonal_periods=s, initialization_method=\"estimated\")\n",
    "    \n",
    "    model = model.fit()\n",
    "\n",
    "    # get metrics and predictions\n",
    "    predictions = model.forecast(len(test_data))\n",
    "    test_predictions[f\"{MODEL_NAME}_{col}_mean\"] = predictions.values\n",
    "\n",
    "test_predictions_df = pd.DataFrame(test_predictions, index=test_data.index)\n",
    "\n",
    "test_predictions_df.to_csv(f\"{str(RESULTS_DIRECTORY)}/predictions.csv\", index=True)\n",
    "print(test_predictions_df.shape)\n",
    "test_predictions_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6fd0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate\n",
    "\n",
    "Let's compute the metrics by comparing the predictions with that of the target data. Note that we will have to rename the columns of the dataframe to match the expected column names by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a5730-c565-44ad-9deb-6f6ac775b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MASE metrics\n",
    "model_metrics, records = utils.get_mase_metrics(\n",
    "    historical_data=train_val_data,\n",
    "    test_predictions=test_predictions_df.rename(\n",
    "            columns={x:x.split(\"_\")[1] for x in test_predictions_df.columns\n",
    "        }),\n",
    "    target_data=test_data,\n",
    "    forecasting_horizons=FORECASTING_HORIZON,\n",
    "    columns=data.columns, \n",
    "    model_name=MODEL_NAME,\n",
    ")\n",
    "\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "records.to_csv(f\"{str(RESULTS_DIRECTORY)}/metrics.csv\", index=False)\n",
    "records[['col', 'horizon', 'mase']].pivot(index=['horizon'], columns='col')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444e2c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd163b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_results(path=DIRECTORY_PATH_TO_SAVE_RESULTS, metric='mase')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edc73c-46b6-46bd-b891-4589a49734f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Plot Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = utils.plot_forecasts(\n",
    "    historical_data=train_val_data,\n",
    "    forecast_directory_path=DIRECTORY_PATH_TO_SAVE_RESULTS,\n",
    "    target_data=test_data,\n",
    "    columns=data.columns,\n",
    "    n_history_to_plot=10, \n",
    "    forecasting_horizon=MAX_FORECASTING_HORIZON,\n",
    "    dpi=200,\n",
    "    plot_se=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201baa6-fcc3-49b1-8afa-2ebf2d61a742",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Conclusions\n",
    "\n",
    "We learned about exponential smoothing models, searched for the best seasonal period for 8 time series in the dataset, and finally, evalauted its performance using the MASE metric. We compared it with ARIMA models' performance in the previous notebook. \n",
    "\n",
    "--- \n",
    "\n",
    "## Exercises\n",
    "\n",
    "- Perform a wider hyperparameter search on the parameters for `ExponentialSmoothing` ([documentation](https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html))\n",
    "\n",
    "- Apply a normalization procedure (e.g., **min-max scaling**) to the data, ensuring that only the training data is used for fitting the scaler. Perform the modeling process on the normalized data and, after generating the final model's predictions, invert the normalization to return the output to its original scale. See `sklearn.preprocessing.MinMaxScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))\n",
    "\n",
    "- Additionally, perform the modeling on the **raw data**, without applying any transformation (such as converting it into log daily returns), to compare results directly with the untransformed dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- To learn about Vector Autoregression method to model multivariate time series, proceed to notebook 3.4\n",
    "\n",
    "- To learn about other machine learning based approaches, check out the module 4 (XGBoost), module 5 (LSTM-based models), module 6 (Transformer based models), or module 7 (LLM-based models).\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
