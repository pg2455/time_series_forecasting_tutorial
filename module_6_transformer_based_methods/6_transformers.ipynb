{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for Forecasting: Informer\n",
    "\n",
    "**Approximate Learning Time**: Up to 4 hours\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will briefly touch on Transformers and their applicability for time series forecasting. Additionally, we will explore a specific type of transformer called Informer. We will use the Hugging Face implementation of Informer and leverage the Lightning framework to train our models.\n",
    "\n",
    "Training transformers can be computationally intensive, so I have reduced the network parameters to create a lightweight version that can be trained with limited computational resources.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Transformers & Informer Model\n",
    "\n",
    "<ins>**What is a Transformer?**</ins>\n",
    "\n",
    "The **Transformer** ([Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762)) is a deep learning architecture that was originally introduced for tasks in natural language processing (NLP), particularly for sequence-to-sequence problems like translation. The key innovation in transformers is the **self-attention mechanism**, which allows the model to weigh the importance of different elements in a sequence, regardless of their distance from each other. This removes the need for sequential data processing (as in RNNs or LSTMs), allowing transformers to process entire sequences in parallel and capture long-range dependencies more efficiently.\n",
    "\n",
    "There are some excellent resources (e.g., [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)) which can be referred to for better understanding of Transformers. \n",
    "\n",
    "<ins>**How are Transformers Suitable for Forecasting?**</ins>\n",
    "\n",
    "While transformers were first developed for NLP tasks, their ability to model long-range dependencies makes them well-suited for **time series forecasting**. Time series data often contain complex relationships across various time scales, which transformers can capture effectively due to their self-attention mechanism. Additionally, transformers can handle irregularly spaced data (e.g., missing data in time series) and multivariate series, making them versatile for forecasting tasks.\n",
    "\n",
    "However, transformers can be resource-intensive when applied to long sequences, which leads to the need for specialized transformer models tailored for time series.\n",
    "\n",
    "<ins>**What is the Informer Model?**</ins>\n",
    "\n",
    "The **Informer** model is a transformer architecture specifically designed for long-sequence time series forecasting. Proposed by [Zhou et al. (2021)](https://arxiv.org/abs/2012.07436), Informer tackles the scalability issues of traditional transformers by introducing two key innovations:\n",
    "- **ProbSparse self-attention**, which reduces the computational complexity of the attention mechanism by focusing on the most informative time steps, rather than attending to every single step in the sequence.\n",
    "- **Distillation**: Informer further improves efficiency by compressing the input sequence into a shorter, more informative representation, which makes it lightweight and fast for long time series forecasting.\n",
    "\n",
    "These improvements make the Informer model particularly well-suited for handling long-range dependencies in large-scale time series data while maintaining efficiency and accuracy.\n",
    "\n",
    "<ins>**What Are Other Transformer Models for Time Series Forecasting?**</ins>\n",
    "\n",
    "There have been many recent proposals aimed at improving transformer architectures for time series forecasting. A non-exhaustive list includes:\n",
    "\n",
    "- **Autoformer** by Wu et al. (2021): This model introduces decomposition-based forecasting, breaking time series into trend and seasonal components, allowing the model to focus on learning these distinct patterns.\n",
    "  \n",
    "- **PatchTST** by Nie et al. (2022): This model leverages the concept of patching, similar to vision transformers, to capture more localized patterns in time series data.\n",
    "  \n",
    "- **Crossformer** by Zhang et al. (2023): Transformer-based model that is designed to explicitly capture cross-dimension dependency.\n",
    "\n",
    "You can find implementations of these models and many more in several time series forecasting libraries:\n",
    "\n",
    "- [**GluonTS**](https://ts.gluon.ai/stable/index.html): A comprehensive library offering several [models](https://ts.gluon.ai/stable/getting_started/models.html) widely used in the academic community.\n",
    "- [**Pytorch Forecasting**](https://pytorch-forecasting.readthedocs.io/en/stable/index.html): A popular library built on top of PyTorch, designed specifically for time series forecasting.\n",
    "- [**Neuralforecast**](https://github.com/Nixtla/neuralforecast): A library that provides several deep learning-based forecasting algorithms.\n",
    "- [**HuggingFace Transformers**](https://huggingface.co/docs/transformers/index): Offers a wide range of transformer models and utilities. We will be using this library in the current notebook to implement the **Informer** model.\n",
    "\n",
    "**References**:\n",
    "\n",
    "[(Vaswani et al. 2017) Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "[(Zhou et al. 2020) Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436)\n",
    "\n",
    "[(Wu et al. 2021) Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008)\n",
    "\n",
    "[(Nie et al. 2022) A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730)\n",
    "\n",
    "[(Zhang et al. 2023) Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting](https://openreview.net/forum?id=vSVLM2j9eie)\n",
    "\n",
    "--- \n",
    "\n",
    "## Probabilistic Forecasting \n",
    "\n",
    "\n",
    "So far, we have focused on **point forecasting techniques**, where models predict a single value for future time steps. However, understanding the **uncertainty** around these estimates is equally important, as it provides confidence in the predictions. This is where **probabilistic forecasting** comes in.\n",
    "\n",
    "In probabilistic forecasting, instead of predicting a single value, the model estimates the **distribution** of possible outcomes. For example, a common approach is to assume a **normal distribution** for the forecast. The model's task is then to predict the parameters of this distributionâ€”specifically, the **mean** and **variance**. \n",
    "\n",
    "Once the model predicts these parameters, we can compute the **likelihood** of observing the actual target value under the predicted distribution. In the case of distributions like the **normal** or **Student's t-distribution**, calculating the likelihood is relatively straightforward. The model then minimizes the **negative log-likelihood (NLL)** of the target values, which serves as the loss function.\n",
    "\n",
    "Because these likelihood functions are smooth and differentiable, **backpropagation** can be applied to update the model weights. This allows the model to learn and improve its predictions over time using standard gradient-based optimization techniques.\n",
    "\n",
    "---\n",
    "\n",
    "We will train the **Informer model**, which by default is implemented to compute the **negative log-likelihood (NLL)** assuming one of the several available distributions provided by Hugging Face. You can explore the available distributions for time series forecasting in the [Hugging Face documentation](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import torch\n",
    "\n",
    "import lightning as L \n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# gluon's import \n",
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "\n",
    "from gluonts.dataset.field_names import FieldName \n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddTimeFeatures, \n",
    "    AddObservedValuesIndicator,\n",
    "    Chain,\n",
    "    RemoveFields,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    "    AsNumpyArray,\n",
    "    InstanceSplitter,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    TestSplitSampler,\n",
    "    ValidationSplitSampler, \n",
    ")\n",
    "\n",
    "from gluonts.itertools import Cyclic\n",
    "from gluonts.dataset.loader import as_stacked_batches\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "\n",
    "from transformers import InformerConfig, InformerForPrediction\n",
    "LAGS_SEQUENCE = [1, 2, 4, 12, 24] # we add these lags as current features.\n",
    "\n",
    "## WARNING: To compare different models on the same horizon, keep this same across the notebooks\n",
    "from termcolor import colored\n",
    "import sys; sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "FORECASTING_HORIZON = [4, 8, 12] # weeks \n",
    "MAX_FORECASTING_HORIZON = max(FORECASTING_HORIZON)\n",
    "\n",
    "SEQUENCE_LENGTH = 2 * MAX_FORECASTING_HORIZON\n",
    "PREDICTION_LENGTH = MAX_FORECASTING_HORIZON\n",
    "\n",
    "DIRECTORY_PATH_TO_SAVE_RESULTS = pathlib.Path('../results/DIY/').resolve()\n",
    "MODEL_NAME = \"Informer\"\n",
    "\n",
    "RESULTS_DIRECTORY = DIRECTORY_PATH_TO_SAVE_RESULTS / MODEL_NAME\n",
    "if RESULTS_DIRECTORY.exists():\n",
    "    print(colored(f'Directory {str(RESULTS_DIRECTORY)} already exists.'\n",
    "           '\\nThis notebook will overwrite results in the same directory.'\n",
    "           '\\nYou can also create a new directory if you want to keep this directory untouched.'\n",
    "           ' Just change the `MODEL_NAME` in this notebook.\\n', \"red\" ))\n",
    "else:\n",
    "    RESULTS_DIRECTORY.mkdir(parents=True)\n",
    "\n",
    "data, transformed_data = utils.load_tutotrial_data(dataset='exchange_rate', log_transform=True)\n",
    "\n",
    "data = transformed_data\n",
    "train_val_data = data.iloc[:-MAX_FORECASTING_HORIZON]\n",
    "train_data, val_data = train_val_data.iloc[:-MAX_FORECASTING_HORIZON], train_val_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "test_data = data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "print(f\"Number of steps in training data: {len(train_data)}\\nNumber of steps in validation data: {len(val_data)}\\nNumber of steps in test data: {len(test_data)}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Transform & Batch Data using GluonTS\n",
    "\n",
    "This section builds on concepts from **Module 1 (Notebook 1.3)** and **Module 2 (Notebook 2.0)**. Specifically, we will:\n",
    "- Leverage the **transformation capabilities** in **GluonTS** that were covered in Module 1/Notebook 1.3, and\n",
    "- Implement a **sampling strategy** to create training, validation, and testing subsets, as introduced in Module 2/Notebook 2.0.\n",
    "\n",
    "--- \n",
    "\n",
    "Let's first convert the data into a `PandasDataset` to match GluonTS's internal dataset representation. Apply the following transformations using GluonTS:\n",
    "\n",
    "- Remove unused fields from the dataset.\n",
    "- Convert data into NumPy arrays.\n",
    "- Add a field `\"observed_values\"`, with 1s for as many target values are present in the sample.\n",
    "- Add time and age features to the dataset, and stack them into a single field, `\"feat_time\"`.\n",
    "- Finally, rename the columns to match the format expected by the Hugging Face transformer.\n",
    "\n",
    "**Note**: The **age feature** is particularly important because it provides the transformer with information about the **position** of each input in the sequence, helping the model understand the temporal context. In **NLP tasks**, transformers use **positional encodings** to capture the order of words in a sentence, as they process input in parallel and do not inherently understand sequence order. Similarly, in time series forecasting, the age feature acts as a positional signal, ensuring that the model understands where each input falls within the time sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into PandasDataset\n",
    "gluon_train_data = PandasDataset(train_data, target=data.columns)\n",
    "gluon_train_val_data = PandasDataset(train_val_data, target=data.columns)\n",
    "gluon_data = PandasDataset(data, target=data.columns)\n",
    "\n",
    "# Define transformations\n",
    "remove_field_names=[FieldName.FEAT_STATIC_REAL, FieldName.FEAT_DYNAMIC_REAL, FieldName.FEAT_STATIC_CAT]\n",
    "transformation = Chain(\n",
    "    [RemoveFields(field_names=remove_field_names)]\n",
    "    + [\n",
    "        AsNumpyArray(\n",
    "            field=FieldName.TARGET,\n",
    "            expected_ndim=2,\n",
    "        ),\n",
    "\n",
    "        AddObservedValuesIndicator(\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.OBSERVED_VALUES,\n",
    "        ),\n",
    "        AddTimeFeatures(\n",
    "            start_field=FieldName.START,\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.FEAT_TIME,\n",
    "            time_features=time_features_from_frequency_str(data.index.freq),\n",
    "            pred_length=MAX_FORECASTING_HORIZON,\n",
    "        ),\n",
    "        AddAgeFeature(\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.FEAT_AGE,\n",
    "            pred_length=MAX_FORECASTING_HORIZON,\n",
    "            log_scale=True,\n",
    "        ),\n",
    "        VstackFeatures(\n",
    "            output_field=FieldName.FEAT_TIME,\n",
    "            input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
    "        ),\n",
    "        RenameFields(\n",
    "            mapping={\n",
    "                FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                FieldName.FEAT_TIME: \"time_features\",\n",
    "                FieldName.TARGET: \"values\",\n",
    "                FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we will use `InstanceSplitter` to define how to split the time series. We want each sample to contain a total of `SEQUENCE_LENGTH + max(LAGS_SEQUENCE)` elements. While the input to the Informer will be `SEQUENCE_LENGTH` long for past time steps, we need `max(LAGS_SEQUENCE)` extra steps to create lagged features for the last time step in the input. Essentially, `InstanceSplitter` samples only the required time steps for featurizing, not the full input.\n",
    "\n",
    "Additionally, it will sample the target over the next `PREDICTION_LENGTH` time steps. The values are passed with separate keys, prefixed with `past_` for input and `future_` for the target. In cases where there arenâ€™t enough observations to fill up `SEQUENCE_LENGTH`, the `observed_values` indicator will fill those positions with 0. The `observed_values` field is processed similarly to the values themselves.\n",
    "\n",
    "The `InstanceSplitter` requires an `InstanceSampler`, which controls which indices to sample. For example:\n",
    "- `ValidationSplitSampler` iterates over indices for validation.\n",
    "- `ExpectedNumInstanceSampler` performs rebalancing to ensure time steps are equally represented in the output.\n",
    "\n",
    "Additionally, we will use the function `as_stacked_batches` ([documentation here](https://ts.gluon.ai/dev/api/gluonts/gluonts.dataset.loader.html?highlight=as_stac#gluonts.dataset.loader.as_stacked_batches)), which serves a similar purpose to `torch.utils.data.DataLoader` by batching the data instances. It also provides other functionalities, such as retaining specific field names during batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_INPUT_NAMES = [\n",
    "    \"past_time_features\",\n",
    "    \"past_values\",\n",
    "    \"past_observed_mask\",\n",
    "    \"future_time_features\",\n",
    "\n",
    "]\n",
    "\n",
    "TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "# TRAINING DATA\n",
    "transformed_data = transformation.apply(gluon_train_data, is_train=True)\n",
    "stream = Cyclic(transformed_data).stream() # never stop serving training data\n",
    "instance_splitter = InstanceSplitter(\n",
    "    target_field=\"values\",\n",
    "    is_pad_field=FieldName.IS_PAD,\n",
    "    start_field=FieldName.START,\n",
    "    forecast_start_field=FieldName.FORECAST_START,\n",
    "    instance_sampler=ExpectedNumInstanceSampler(num_instances=1.0, min_future=MAX_FORECASTING_HORIZON),\n",
    "    past_length=SEQUENCE_LENGTH + max(LAGS_SEQUENCE),\n",
    "    future_length=PREDICTION_LENGTH,\n",
    "    time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    ")\n",
    "training_instances = instance_splitter.apply(stream) # applies the above specified logic in the instance splitter to stream\n",
    "\n",
    "batch_size=64\n",
    "train_loader = as_stacked_batches(\n",
    "        training_instances,\n",
    "        batch_size=batch_size,\n",
    "        field_names=TRAINING_INPUT_NAMES,\n",
    "        output_type=torch.tensor,\n",
    "        num_batches_per_epoch=100\n",
    "    )\n",
    "\n",
    "# VALIDATION DATA\n",
    "transformed_data = transformation.apply(gluon_train_val_data, is_train=True)\n",
    "instance_splitter = InstanceSplitter(\n",
    "    target_field=\"values\",\n",
    "    is_pad_field=FieldName.IS_PAD,\n",
    "    start_field=FieldName.START,\n",
    "    forecast_start_field=FieldName.FORECAST_START,\n",
    "    instance_sampler=ValidationSplitSampler(min_future=MAX_FORECASTING_HORIZON),\n",
    "    past_length=SEQUENCE_LENGTH + max(LAGS_SEQUENCE),\n",
    "    future_length=PREDICTION_LENGTH,\n",
    "    time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    ")\n",
    "val_instances = instance_splitter.apply(transformed_data, is_train=True)\n",
    "val_loader = as_stacked_batches(\n",
    "        val_instances,\n",
    "        batch_size=batch_size,\n",
    "        field_names=TRAINING_INPUT_NAMES,\n",
    "        output_type=torch.tensor,\n",
    "    )\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "print(batch.keys())\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape, v.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Check the field names and the corresponding tensor shape. Are they according to what you expected?\n",
    "\n",
    "---\n",
    "\n",
    "## Informer model using ðŸ¤— Transformers\n",
    "\n",
    "\n",
    "ðŸ¤— Transformers provides an API that makes it easy to download and train state-of-the-art transformer models. The library also includes an implementation of the Informer model ([documentation here](https://huggingface.co/docs/transformers/main/en/model_doc/informer)), which we will use.\n",
    "\n",
    "The model consists of three main classes:\n",
    "- `InformerConfig`: Defines the model configuration.\n",
    "- `InformerModel`: The actual implementation of the model, initialized based on the configuration.\n",
    "- `InformerForPrediction`: Loads the pre-trained transformer model and can be used for inference.\n",
    "\n",
    "We will define the configuration and train the model using the Lightning framework. Note that the default implementation of Informer outputs a probability distribution instead of using L2 loss or L1 loss like in the previous modules.\n",
    "\n",
    "Additionally, we will perform a dry run with a batch from the data loader to ensure the batches are being processed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Time Series Transformer configuration with 12 time steps for prediction\n",
    "configuration = InformerConfig(\n",
    "    prediction_length=PREDICTION_LENGTH,\n",
    "    context_length=SEQUENCE_LENGTH,\n",
    "    lags_sequence=LAGS_SEQUENCE,\n",
    "    num_time_features=3,\n",
    "    distribution_output='student_t', # can be `normal` as well \n",
    "    input_size=8,\n",
    "    d_model=32, # reduceed to make it lightweight \n",
    "    encoder_layers=2, # reduceed to make it lightweight \n",
    "    decoder_layers=2,\n",
    "    dropout=0.1,\n",
    "    num_parallel_samples=100, \n",
    ")\n",
    "\n",
    "# Randomly initializing a model (with random weights) from the configuration\n",
    "model = InformerForPrediction(configuration)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "outputs = model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    "    future_observed_mask=batch[\"future_observed_mask\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have a working implementation of the Informer model, and weâ€™re able to compute the negative log-likelihood of our outputs. The next step is to begin training the model and learning the weights.\n",
    "\n",
    "--- \n",
    "\n",
    "## Training the model\n",
    "\n",
    "Following the same structure as the previous notebooks, we will initialize the LightningModule and Trainer from pytorch_lighnint and let it run the training and validation loops.\n",
    "\n",
    "Here, we will experiment by looping over two different values of n_encoders and train the model for a few steps, keeping in mind the relatively slow training speed of transformers. If training is too slow on your system, feel free to reduce the model parameters even further.\n",
    "\n",
    "**Note**: The method `_shared_eval` has been added to the module for this implementation. This method is used exclusively during **inference** and has no role during training. We will revisit its purpose and functionality during the **Evaluation** section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningInformer(L.LightningModule):\n",
    "    def __init__(self, informer_config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.informer = InformerForPrediction(informer_config)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.informer(\n",
    "            past_values=batch[\"past_values\"],\n",
    "            past_time_features=batch[\"past_time_features\"],\n",
    "            future_values=batch[\"future_values\"],\n",
    "            future_time_features=batch[\"future_time_features\"],\n",
    "            future_observed_mask=batch[\"future_observed_mask\"],\n",
    "            past_observed_mask=batch[\"past_observed_mask\"],\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch)\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Note: one can evaluate using the median and self.informer.generate by using _shared_eval\n",
    "        # For computational efficiency, we will stick to the loss function here\n",
    "        outputs = self(batch)\n",
    "        loss = outputs.loss\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def _shared_eval(self, batch, device):\n",
    "        self.informer.eval()\n",
    "\n",
    "        return self.informer.to(device).generate(\n",
    "            past_time_features=batch[\"past_time_features\"].to(device),\n",
    "            past_values=batch[\"past_values\"].to(device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss, best_params = np.inf, []\n",
    "for n_encoders in [2, 4]:\n",
    "    seed_everything(42, workers=True)\n",
    "\n",
    "\n",
    "    # define callbacks \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=True, patience=10)\n",
    "\n",
    "    # save the best checkpoint; this will be used to evaluate test metrics\n",
    "    best_checkpoint = ModelCheckpoint(save_top_k=1, monitor='val_loss',\n",
    "                                    mode='min', \n",
    "                                    filename='{epoch:02d}-{global_step}-{val_loss:.5f}')\n",
    "    # save the last checkpoint in case if you want to resume training \n",
    "    last_checkkpoint =  ModelCheckpoint(save_top_k=1, monitor='global_step',\n",
    "                                    mode='max',\n",
    "                                    filename='{epoch:02d}-{global_step}')\n",
    "\n",
    "    configuration.encoder_layers = n_encoders\n",
    "    model = LightningInformer(configuration)\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=500, \n",
    "        val_check_interval=20,\n",
    "        callbacks=[early_stopping, best_checkpoint], \n",
    "        deterministic=True,\n",
    "        )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    if best_val_loss > best_checkpoint.best_model_score:\n",
    "        best_val_loss = best_checkpoint.best_model_score\n",
    "        best_params = (n_encoders, )\n",
    "\n",
    "\n",
    "print(f\"\\n\\nBest params: encoder_layers: {best_params[0]}. Val loss: {best_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the training trajectory**\n",
    "\n",
    "Run the following command on command line and navigate to the port on which tensorboard is launched. Then navigate to `Scalars` tab to see how `train_loss` and `val_loss` changes during the training. \n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=lightning_logs\n",
    "```\n",
    "--- \n",
    "\n",
    "## Refit on Train-Val Subset\n",
    "\n",
    "To evaluate the model's performance on the test data, we will first retrain the model using the best hyperparameters identified earlier, this time utilizing the combined train-validation dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit on train and val dataset\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "\n",
    "# training data\n",
    "transformed_data = transformation.apply(gluon_train_val_data, is_train=True)\n",
    "stream = Cyclic(transformed_data).stream()\n",
    "instance_splitter = InstanceSplitter(\n",
    "    target_field=\"values\",\n",
    "    is_pad_field=FieldName.IS_PAD,\n",
    "    start_field=FieldName.START,\n",
    "    forecast_start_field=FieldName.FORECAST_START,\n",
    "    instance_sampler=ExpectedNumInstanceSampler(num_instances=1.0, min_future=MAX_FORECASTING_HORIZON),\n",
    "    past_length=SEQUENCE_LENGTH + max(LAGS_SEQUENCE),\n",
    "    future_length=PREDICTION_LENGTH,\n",
    "    time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    ")\n",
    "train_val_instances = instance_splitter.apply(stream)\n",
    "\n",
    "batch_size=64\n",
    "train_val_loader = as_stacked_batches(\n",
    "        train_val_instances,\n",
    "        batch_size=batch_size,\n",
    "        field_names=TRAINING_INPUT_NAMES,\n",
    "        output_type=torch.tensor,\n",
    "        num_batches_per_epoch=100\n",
    "    )\n",
    "\n",
    "# define the best obtained configuration\n",
    "configuration.encoder_layers = best_params[0]\n",
    "model = LightningInformer(configuration)\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=30, \n",
    "    callbacks=[EarlyStopping(monitor='train_loss', mode='min', verbose=True, patience=5)], \n",
    "    deterministic=True\n",
    "    )\n",
    "\n",
    "trainer.fit(model, train_val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Forecast\n",
    "\n",
    "As we observed in the previous module, predictions in this model are also generated iteratively. We will implement this process within the `_shared_eval` function to handle iterative prediction generation.\n",
    "\n",
    "\n",
    "Finally, although we are learning a **distribution**, at test time the outputs are **sampled from the predicted distribution**. This sampling is performed iteratively.\n",
    "\n",
    "For evaluation, we will use the **median** of the distribution as it provides a more robust estimate. However, feel free to experiment with using the **mean** or any other statistic to evaluate the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the best model\n",
    "model = LightningInformer.load_from_checkpoint(\n",
    "    checkpoint_path=trainer.checkpoint_callback.best_model_path)\n",
    "model.eval()\n",
    "\n",
    "transformed_data = transformation.apply(gluon_data, is_train=False)\n",
    "\n",
    "instance_splitter = InstanceSplitter(\n",
    "    target_field=\"values\",\n",
    "    is_pad_field=FieldName.IS_PAD,\n",
    "    start_field=FieldName.START,\n",
    "    forecast_start_field=FieldName.FORECAST_START,\n",
    "    instance_sampler=TestSplitSampler(),\n",
    "    past_length=SEQUENCE_LENGTH + max(LAGS_SEQUENCE),\n",
    "    future_length=PREDICTION_LENGTH,\n",
    "    time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    ")\n",
    "test_instances = instance_splitter.apply(transformed_data, is_train=False)\n",
    "\n",
    "batch_size=64\n",
    "test_loader = as_stacked_batches(\n",
    "        test_instances,\n",
    "        batch_size=batch_size,\n",
    "        field_names=PREDICTION_INPUT_NAMES,\n",
    "        output_type=torch.tensor,\n",
    "    )\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "\n",
    "# iterative prediction\n",
    "forecasts = model._shared_eval(batch, device=torch.device('cpu'))[0].squeeze(0)\n",
    "\n",
    "# the model outputs a distribution so we keep a point estimate here.\n",
    "forecast_median = np.median(forecasts, 0)\n",
    "\n",
    "AUGMENTED_COL_NAMES = [f\"{MODEL_NAME}_{col}_mean\" for col in data.columns]\n",
    "test_predictions_df = pd.DataFrame(forecast_median, columns=AUGMENTED_COL_NAMES, index=test_data.index)\n",
    "\n",
    "# ssave them to the directory\n",
    "test_predictions_df.to_csv(f\"{str(RESULTS_DIRECTORY)}/predictions.csv\", index=True)\n",
    "print(test_predictions_df.shape)\n",
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Evaluate \n",
    "\n",
    "Let's compute the metrics by comparing the predictions with that of the target data. Note that we will have to rename the columns of the dataframe to match the expected column names by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalaute metrics\n",
    "target_data = data[-MAX_FORECASTING_HORIZON:]\n",
    "model_metrics, records = utils.get_mase_metrics(\n",
    "    historical_data=train_val_data,\n",
    "    test_predictions=test_predictions_df.rename(\n",
    "            columns={x:x.split(\"_\")[1] for x in test_predictions_df.columns\n",
    "        }),\n",
    "    target_data=target_data,\n",
    "    forecasting_horizons=FORECASTING_HORIZON,\n",
    "    columns=data.columns, \n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "records.to_csv(f\"{str(RESULTS_DIRECTORY)}/metrics.csv\", index=False)\n",
    "records[['col', 'horizon', 'mase']].pivot(index=['horizon'], columns='col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_results(path=DIRECTORY_PATH_TO_SAVE_RESULTS, metric='mase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Plot Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = utils.plot_forecasts(\n",
    "    historical_data=train_val_data,\n",
    "    forecast_directory_path=DIRECTORY_PATH_TO_SAVE_RESULTS,\n",
    "    target_data=target_data,\n",
    "    columns=data.columns,\n",
    "    n_history_to_plot=10, \n",
    "    forecasting_horizon=MAX_FORECASTING_HORIZON,\n",
    "    dpi=200,\n",
    "    exclude_models=['LSTM', 'ExpSmooth', 'VAR'],\n",
    "    plot_se=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We gained an understanding of how transformers work and their applicability to probabilistic time series forecasting. Specifically, we explored the Informer model, and successfully trained one using the Hugging Face Transformers API and the Lightning framework.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "- Find MASE using the mean instead of the median. What differences do you observe in the results?\n",
    "  \n",
    "- Plot the 1-standard deviation confidence intervals around the predictions.\n",
    "  \n",
    "- Change the output distribution type and analyze how it affects the predictions.\n",
    "  \n",
    "- Select a few hyperparameters and perform hyperparameter optimization to improve the model.\n",
    "  \n",
    "- Explore other time series transformers available in Hugging Face and compare their performance.\n",
    "  \n",
    "- Add more features to the model and assess their impact on forecasting accuracy.\n",
    "  \n",
    "- Apply a normalization procedure (e.g., **min-max scaling**) to the data, ensuring that only the training data is used for fitting the scaler. Perform the modeling process on the normalized data and, after generating the final model's predictions, invert the normalization to return the output to its original scale. See `sklearn.preprocessing.MinMaxScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))\n",
    "  \n",
    "- Additionally, perform the modeling on the **raw data**, without applying any transformation (such as converting it into log daily returns), to compare results directly with the untransformed dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Proceed to the next module to learn about LLM-based approaches for time series forecasting. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chains1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
