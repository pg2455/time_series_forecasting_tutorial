{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-based Approaches for Forecasting: Chronos\n",
    "\n",
    "**Approximate Learning Time**:Up to 2 hours\n",
    "\n",
    "---\n",
    "\n",
    "In this module, we will explore LLM-based approaches for time series forecasting. LLMs (Large Language Models) are trained on vast amounts of online data, and due to their sequential nature, they are inherently effective at predicting future steps—making them a promising candidate for forecasting tasks. As a result, there have been several attempts to leverage LLMs for time series forecasting.\n",
    "\n",
    "Broadly, the community has proposed three types of approaches:\n",
    "\n",
    "1. **Directly prompting pre-trained LLMs** (e.g., GPT-4, LLaMa): \n",
    "   This method involves using pre-trained LLMs for forecasting by simply prompting them. While we will discuss this approach here, the hands-on tutorial for it is left as an exercise in the folder LLMTime. The tutorial is based on the work **LLMTime** ([Gruver et al. (2023)](https://arxiv.org/pdf/2310.07820)), though it uses a different dataset and requires API keys. You are encouraged to follow that tutorial and apply it to the dataset we've been using throughout this tutorial.\n",
    "\n",
    "2. **Training LLMs directly on massive time series datasets**:\n",
    "   This approach involves using LLMs specifically trained for time series forecasting, as seen in models like **Chronos** ([Ansari et al. (2024)](https://arxiv.org/abs/2403.07815)), which we will cover in this notebook. These models are used in a way similar one would prompt pre-trained LLMs. We will compare results from **Chronos** with other models we have explored so far.\n",
    "\n",
    "3. **Reprogramming LLMs like GPT-2**:\n",
    "   The focus of the next notebook is on reprogramming existing LLMs for time series forecasting. We will discuss **TimeLLM**, a recently published approach by [Jin et al. (2023)](https://arxiv.org/abs/2310.01728) where the authors reprogram open source models like GPT-2 to function as time series forecasters. Due to the computational requirements of this approach—such as loading large models like GPT-2—we will only briefly touch on the implementation in the next notebook. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## In-Context Learning\n",
    "\n",
    "Large Language Models (LLMs) are built using transformer architectures. After being trained on a vast corpus of text to predict the next token in a sequence, these models are ready to perform inference based on the input provided to them.\n",
    "\n",
    "We interact with LLMs by inputting text and receiving a generated response. For example, if we want to perform sentiment classification, we can prompt the model in different ways:\n",
    "\n",
    "<ins>**Zero-shot learning**</ins>: The model completes the task without any prior examples or guidance.\n",
    "\n",
    "```txt\n",
    "Classify the following sentence as either negative or positive:\n",
    "\"I received a broken chair.\"\n",
    "```\n",
    "\n",
    "In this case, the LLM may not have seen this exact task during training, but it can still figure out how to perform it. This is why it's called “learning” – the LLM generalizes to a task it hasn't directly encountered before. Zero-shot learning is useful for tasks that the model may not have explicitly seen during training.\n",
    "\n",
    "In <ins>**few-shot learning**</ins>, the prompt includes a few examples to guide the model. Here's an example of a 2-shot learning prompt:\n",
    "\n",
    "```txt\n",
    "You are an AI assistant tasked with classifying the sentiment of sentences.\n",
    "You should respond with \"Positive\" or \"Negative\".\n",
    "\n",
    "Here are some examples:\n",
    "Sentence: \"I had a bad experience.\"\n",
    "Your response: Negative\n",
    "\n",
    "Sentence: \"I had a blast!\"\n",
    "Your response: Positive\n",
    "\n",
    "Now, classify this sentence:\n",
    "\"I received a broken chair.\"\n",
    "```\n",
    "\n",
    "LLMs demonstrate a surprising ability to learn from context within the prompt. Although we don't have a concrete theory on how LLMs perform in-context learning, it’s speculated that they build a generalized world model during training, which allows them to adapt quickly to unseen tasks.\n",
    "\n",
    "--- \n",
    "\n",
    "## Zero-shot Forecasting\n",
    "\n",
    "In this tutorial, we'll explore how LLMs can be used for time series forecasting through prompting. Time series data can be converted into a tokenized form that LLMs expect as input. LLMs, trained on billions of words and capable of predicting the next token in a sequence, can use their predictive abilities for time series forecasting.\n",
    "\n",
    "Recently, researchers have reframed time series forecasting as a token prediction task. This means that time series data is tokenized, fed into the LLM, and the model predicts future tokens, which are then decoded back into numerical values.\n",
    "\n",
    "The detailed process includes:\n",
    "1. **Scaling**: Time series values are scaled (e.g., using min-max scaling) to keep values within a suitable range.\n",
    "   \n",
    "2. **Tokenization**: The scaled values are tokenized into strings that LLMs can understand. This tokenization process is specific to the LLM being used.\n",
    "\n",
    "3. **Prediction**: The LLM predicts the next tokens based on the input sequence. In this framework, **forecasting is reframed as a classification task**, where the LLM selects the most likely next tokens from a distribution.\n",
    "   \n",
    "4. **Decoding**: The predicted tokens are then decoded back into string.\n",
    "\n",
    "5. **Inverse scaling**: Finally, the decoded values are converted back to their original scale through **inverse scaling**, restoring the values to their real-world range.\n",
    "\n",
    "\n",
    "The diagram below illustrates this prompting flow:\n",
    "\n",
    "<div style=\"text-align: center; padding: 20px;\">\n",
    "<img src=\"LLMTime/img/llmtime.png\" style=\"max-width: 70%; clip-path: inset(2px); height: auto; border-radius: 15px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);\">\n",
    "</div>\n",
    "\n",
    "### LLMTime: Zero-shot Forecasting with GPT-4\n",
    "\n",
    "In a recent study, LLMTime by Gruver et al. (2023) used GPT-4 and LLaMA for time series forecasting. These models, trained extensively on online data, were prompted to predict future values in time series data. The authors claim that LLMs prefer simpler explanations, which helps them generate accurate forecasts.\n",
    "\n",
    "The LLMTime tutorial, included in the accompanying folder, uses weather data to predict average maximum temperature. However, you are free to experiment with any dataset of your choice.\n",
    "\n",
    "### Chronos LLM for Zero-shot Forecasting\n",
    "\n",
    "Chronos is an LLM specifically trained on time series data. A large collection of both public and synthetically created time series datasets were scaled and tokenized, similarly to the process described above. Chronos was trained on these quantized time series to predict future values.\n",
    "\n",
    "Unlike GPT-4, Chronos is specfically designed for univariate time series forecasting and ignores time and frequency information, treating the time series purely as a sequence of values.\n",
    "\n",
    "In **Table 2** of the Appendix B in Ansari et al. (2024), you’ll find the datasets Chronos was trained on. They are categorized into:\n",
    "- **Pretraining-only datasets**: Used for model training.\n",
    "- **In-domain evaluation datasets**: Used for partial training, and evaluated on their later steps.\n",
    "- **Zero-shot evaluation datasets**: These datasets were not seen during training.\n",
    "\n",
    "For this tutorial, we are focusing on the **Exchange Rate dataset**, which Chronos did not encounter during training, making it a zero-shot evaluation task.\n",
    "\n",
    "\n",
    "**References**:\n",
    "\n",
    "[(Gruver et al. 2023) Large Language Models Are Zero-Shot Time Series Forecasters](https://arxiv.org/pdf/2310.07820)\n",
    "\n",
    "[(Jin et al. 2023) Time-LLM: Time Series Forecasting by Reprogramming Large Language Models](https://arxiv.org/abs/2310.01728)\n",
    "\n",
    "[(Ansari et al. 2024) Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815)\n",
    "\n",
    "--- \n",
    "\n",
    "Let's load the log daily returns of exchange rates, and split the data into train, validation, and test subsets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "## WARNING: To compare different models on the same horizon, keep this same across the notebooks\n",
    "from termcolor import colored\n",
    "import sys; sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "FORECASTING_HORIZON = [4, 8, 12] # weeks \n",
    "MAX_FORECASTING_HORIZON = max(FORECASTING_HORIZON)\n",
    "\n",
    "PREDICTION_LENGTH = MAX_FORECASTING_HORIZON\n",
    "\n",
    "DIRECTORY_PATH_TO_SAVE_RESULTS = pathlib.Path('../results/DIY/').resolve()\n",
    "MODEL_NAME = \"Chronos\"\n",
    "\n",
    "RESULTS_DIRECTORY = DIRECTORY_PATH_TO_SAVE_RESULTS / MODEL_NAME\n",
    "if RESULTS_DIRECTORY.exists():\n",
    "    print(colored(f'Directory {str(RESULTS_DIRECTORY)} already exists.'\n",
    "           '\\nThis notebook will overwrite results in the same directory.'\n",
    "           '\\nYou can also create a new directory if you want to keep this directory untouched.'\n",
    "           ' Just change the `MODEL_NAME` in this notebook.\\n', \"red\" ))\n",
    "else:\n",
    "    RESULTS_DIRECTORY.mkdir(parents=True)\n",
    "\n",
    "data, transformed_data = utils.load_tutotrial_data(dataset='exchange_rate', log_transform=True)\n",
    "data = transformed_data\n",
    "\n",
    "train_val_data = data.iloc[:-MAX_FORECASTING_HORIZON]\n",
    "train_data, val_data = train_val_data.iloc[:-MAX_FORECASTING_HORIZON], train_val_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "test_data = data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "print(f\"Number of steps in training data: {len(train_data)}\\nNumber of steps in validation data: {len(val_data)}\\nNumber of steps in test data: {len(test_data)}\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Forecast\n",
    "\n",
    "\n",
    "Chronos is a pre-trained LLM specifically designed for time series forecasting. This means we only need to prompt the LLM to generate forecasts, making the process straightforward.\n",
    "\n",
    "For this tutorial, we will use the smaller version of Chronos, which is based on the T5 model ([Wikipedia](https://en.wikipedia.org/wiki/T5_(language_model))). If you are running the code on CPUs, ensure that the device is set to `\"cpu\"`.\n",
    "\n",
    "One important aspect to consider is how Chronos accepts the time series input. The following example is adapted from their [official documentation](https://github.com/amazon-science/chronos-forecasting?tab=readme-ov-file), and we will use it as a reference in our tutorial. Since Chronos performs probabilistic forecasting, it requires specifying the number of samples you want to generate. Keep in mind that this step may take around 2 minutes to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"mps\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# context must be either a 1D tensor, a list of 1D tensors,\n",
    "# or a left-padded 2D tensor with batch as the first dimension\n",
    "# forecast shape: [num_series, num_samples, prediction_length]\n",
    "forecast = pipeline.predict(\n",
    "    context=torch.tensor(train_val_data.transpose().values),\n",
    "    prediction_length=MAX_FORECASTING_HORIZON,\n",
    "    num_samples=100,\n",
    ")\n",
    "\n",
    "# the model outputs a distribution so we keep a point estimate here.\n",
    "forecast = forecast.permute(1, 2, 0)\n",
    "forecast_median = np.median(forecast, 0)\n",
    "\n",
    "AUGMENTED_COL_NAMES = [f\"{MODEL_NAME}_{col}_mean\" for col in data.columns]\n",
    "test_predictions_df = pd.DataFrame(forecast_median, columns=AUGMENTED_COL_NAMES, index=test_data.index)\n",
    "\n",
    "# save them to the directory\n",
    "test_predictions_df.to_csv(f\"{str(RESULTS_DIRECTORY)}/predictions.csv\", index=True)\n",
    "print(test_predictions_df.shape)\n",
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Evaluate \n",
    "\n",
    "Let's compute the metrics by comparing the predictions with that of the target data. Note that we will have to rename the columns of the dataframe to match the expected column names by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalaute metrics\n",
    "target_data = data[-MAX_FORECASTING_HORIZON:]\n",
    "model_metrics, records = utils.get_mase_metrics(\n",
    "    historical_data=train_val_data,\n",
    "    test_predictions=test_predictions_df.rename(\n",
    "            columns={x:x.split(\"_\")[1] for x in test_predictions_df.columns\n",
    "        }),\n",
    "    target_data=target_data,\n",
    "    forecasting_horizons=FORECASTING_HORIZON,\n",
    "    columns=data.columns, \n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "records.to_csv(f\"{str(RESULTS_DIRECTORY)}/metrics.csv\", index=False)\n",
    "records[['col', 'horizon', 'mase']].pivot(index=['horizon'], columns='col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_results(path=DIRECTORY_PATH_TO_SAVE_RESULTS, metric='mase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Plot Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = utils.plot_forecasts(\n",
    "    historical_data=train_val_data,\n",
    "    forecast_directory_path=DIRECTORY_PATH_TO_SAVE_RESULTS,\n",
    "    target_data=target_data,\n",
    "    columns=data.columns,\n",
    "    n_history_to_plot=10, \n",
    "    forecasting_horizon=MAX_FORECASTING_HORIZON,\n",
    "    dpi=200,\n",
    "    exclude_models=['LSTM'],\n",
    "    plot_se=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored two approaches for using LLMs in time series forecasting. First, we learned how to leverage general-purpose LLMs, such as GPT-4 and LLaMA, by framing forecasting as a token prediction task. Then, we applied Chronos, an LLM specifically pre-trained for time series data, to perform forecasting tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "- Check how the mean performs on MASE.\n",
    "\n",
    "- Plot standard errors on the distribution.\n",
    "\n",
    "- Follow the TimeLLM tutorial to perform forecasting using GPT-4 or LLaMA.\n",
    "\n",
    "- Apply a normalization procedure (e.g., **min-max scaling**) to the data, ensuring that only the training data is used for fitting the scaler. Perform the modeling process on the normalized data and, after generating the final model's predictions, invert the normalization to return the output to its original scale. See `sklearn.preprocessing.MinMaxScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))\n",
    "\n",
    "- Additionally, perform the modeling on the **raw data**, without applying any transformation (such as converting it into log daily returns), to compare results directly with the untransformed dataset.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Head to the last notebook in this module to learn how to reprogram LLMs like GPT-2 for time series forecasting.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chains1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
