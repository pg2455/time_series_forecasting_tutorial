{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Methods for Forecasting: LSTNet\n",
    "\n",
    "**Approximate Learning Time**:Up to 4 hours\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will understand LSTNet proposal by [Lai et al., 2018](https://arxiv.org/abs/1703.07015) that addresses the limitations of LSTM models in time series forecasting. The proposed architecture combines Convolutional Networks (CNN), LSTM, and Attention mechanisms to tackle these challenges. By exploring this model, we gain a deeper understanding of how various deep learning architectures can be used and where they excel.\n",
    "\n",
    "---\n",
    "\n",
    "## LSTNet Model\n",
    "\n",
    "\n",
    "Time series often exhibit both short-term and long-term recurring patterns. For example, hourly traffic data might display daily patterns as well as weekly patterns, with reduced traffic on weekends. In more complex time series, there can be multiple patterns occurring at different time scales.\n",
    "\n",
    "The LSTNet model was proposed to capture these patterns by utilizing:\n",
    "- Convolutional Networks (CNN) to model short-term dependencies,\n",
    "- LSTMs (or Gated Recurrent Units, GRUs) to capture long-term dependencies,\n",
    "- Skip connections or attention mechanisms to model very long-term dependencies.\n",
    "\n",
    "Convolutional networks excel at capturing local patterns by sliding a window (or kernel) over the input. This window is applied over the input sequence to extract local features, and different kernels can capture different aspects of the local structure. You can read more about Convolutional Neural Networks [here on Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network).\n",
    "\n",
    "We’ve already studied LSTM in the previous notebook. While LSTMs are effective at modeling long-term dependencies, they still struggle with very long-term dependencies due to the same vanishing gradient problem faced by RNNs. The authors proposed using GRU (Gated Recurrent Unit) instead of LSTM. GRUs are a more compact version of LSTMs, with fewer parameters, making them computationally lighter while offering comparable performance.\n",
    "\n",
    "To address this, the authors of LSTNet introduced LSTM-skip connections, which skip certain elements in the sequence and focus on capturing long-range dependencies. By skipping steps from the last time step, LSTM-skip connections can better capture long-term patterns.\n",
    "\n",
    "Another approach to handle long-term dependencies is the use of the attention mechanism, which was [originally proposed](https://arxiv.org/abs/1409.0473) to overcome the vanishing gradient problem in long sequences. While this mechanism is not depicted in the diagram below, it is discussed in the text version of the LSTNet paper.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center; padding: 20px;\">\n",
    "<img src=\"../images/lstnet.png\" style=\"max-width: 90%; clip-path: inset(2px); height: auto; border-radius: 15px; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1); \"></img>\n",
    "</div>\n",
    "\n",
    "\n",
    "Finally, the LSTNet model includes an autoregressive (AR) component that models the output as a linear autoregression on the last $k$ time steps. This autoregressive component is added to the highly non-linear part of the model, which may not preserve the input scale. The idea is that the linear AR component can act as an output scaler, helping to maintain the appropriate scale of the forecasted values, which might otherwise be lost in the non-linear transformations.\n",
    "\n",
    "**References**:\n",
    "\n",
    "[[Lai et al., 2018] Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks](https://arxiv.org/abs/1703.07015)\n",
    "\n",
    "---\n",
    "\n",
    "Let's load the log daily returns of exchange rates, and split the data into train, validation, and test subsets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F \n",
    "import torch.nn as nn \n",
    "\n",
    "import lightning as L \n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import optuna\n",
    "\n",
    "\n",
    "## WARNING: To compare different models on the same horizon, keep this same across the notebooks\n",
    "import sys; sys.path.append(\"../\")\n",
    "import utils\n",
    "from utils_tutorial import load_file\n",
    "\n",
    "FORECASTING_HORIZON = [4, 8, 12] # weeks \n",
    "MAX_FORECASTING_HORIZON = max(FORECASTING_HORIZON)\n",
    "\n",
    "SEQUENCE_LENGTH = 2 * MAX_FORECASTING_HORIZON\n",
    "PREDICTION_LENGTH = MAX_FORECASTING_HORIZON\n",
    "\n",
    "DIRECTORY_PATH_TO_SAVE_RESULTS = pathlib.Path('../results/DIY/').resolve()\n",
    "MODEL_NAME = \"LSTNet\"\n",
    "\n",
    "RESULTS_DIRECTORY = DIRECTORY_PATH_TO_SAVE_RESULTS / MODEL_NAME\n",
    "if RESULTS_DIRECTORY.exists():\n",
    "    print(f'Directory {str(RESULTS_DIRECTORY)} already exists.'\n",
    "           '\\nThis notebook will overwrite results in the same directory.'\n",
    "           '\\nYou can also create a new directory if you want to keep this directory.'\n",
    "           ' Just change the `MODEL_NAME` in this notebook.\\n')\n",
    "else:\n",
    "    RESULTS_DIRECTORY.mkdir()\n",
    "\n",
    "data, transformed_data = utils.load_tutotrial_data(dataset='exchange_rate', log_transform=True)\n",
    "data = transformed_data\n",
    "\n",
    "train_val_data = data.iloc[:-MAX_FORECASTING_HORIZON]\n",
    "train_data, val_data = train_val_data.iloc[:-MAX_FORECASTING_HORIZON], train_val_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "test_data = data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "print(f\"Number of steps in training data: {len(train_data)}\\nNumber of steps in validation data: {len(val_data)}\\nNumber of steps in test data: {len(test_data)}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Build a rough LSTNet \n",
    "\n",
    "In this section, we will roughly implement the LSTNet model, starting from equations 1 to 6 in the paper by Lai et al. (2018).\n",
    "\n",
    "We will utilize the `SlidingFixedWindow` dataloader from the previous notebook to extract a batch of input data. This batch will serve as the input for implementing the different components of LSTNet, including the convolutional, recurrent, and autoregressive components.\n",
    "\n",
    "I’ve intentionally left parts of the implementation blank for you to practice completing the code yourself. Once you've filled in the blanks, **you can compare your implementation against mine by copying it into the next cell block** and running the function to verify your work.\n",
    "\n",
    "```python\n",
    "%%load_file \n",
    "../solutions/lstnet_rough.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "dataset = utils.SlidingFixedWindow(train_data, seq_length)\n",
    "train_loader = DataLoader(dataset, batch_size=2,  shuffle=True)\n",
    "\n",
    "inputs, targets = next(iter(train_loader))\n",
    "\n",
    "print(\"Inputs shape\", inputs.shape)\n",
    "dropout = 0\n",
    "n_out_channels = 10\n",
    "input_dim = inputs.shape[-1]\n",
    "window=2\n",
    "bs = inputs.shape[0]\n",
    "out_features = targets.shape[-1]\n",
    "\n",
    "# Eq. (1) Convolutional Component\n",
    "conv1 = nn.Conv2d(\n",
    "        in_channels= <BLANK>, \n",
    "        out_channels=n_out_channels, \n",
    "        kernel_size=(window, input_dim)\n",
    "    )\n",
    "\n",
    "h_conv = conv1(inputs.unsqueeze(1)).squeeze(-1)\n",
    "print(\"Conv output shape:\", h_conv.shape)\n",
    "\n",
    "# Eq. (2) Recurrent Component \n",
    "hidden_state_dims_GRU1 = 32\n",
    "GRU1 = nn.GRU(\n",
    "        input_size=<BLANK>,\n",
    "        hidden_size=hidden_state_dims_GRU1,\n",
    "        batch_first=True, \n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "h_conv_in = h_conv.permute(0, 2, 1)\n",
    "print(\"GRU input shape: \", h_conv_in.shape)\n",
    "H_gru, h_gru = GRU1(h_conv_in)\n",
    "h_gru = h_gru.squeeze(0)\n",
    "print(\"GRU output shape:\", h_gru.shape)\n",
    "\n",
    "# Eq. (3) Recurrent-skip Component (GRU for every p hidden states)\n",
    "skip = 4\n",
    "hidden_state_dims_GRU2 = 16\n",
    "GRU2 = nn.GRU(\n",
    "    input_size=<BLANK>,\n",
    "    hidden_size=hidden_state_dims_GRU2,\n",
    "    batch_first=True,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "seq_len = h_conv_in.shape[1] // skip # each sequence will have these many elements\n",
    "n_seq = skip # there will be these many sequences\n",
    "c = h_conv_in[:, -<BLANK>:] # discard the states which can't fit in the window\n",
    "c = c.view(<BLANK>, seq_len, n_seq, c.shape[-1]).contiguous() # stride every n_seq before switching index\n",
    "\n",
    "# switch the dimensions and obtain the input for GRU\n",
    "c = c.permute(0, 2, 1, 3).contiguous().view(bs*n_seq, <BLANK>, c.shape[-1])\n",
    "\n",
    "print(\"These must be equal: \", c[1, :, 1], h_conv_in[0, 2::skip, 1])\n",
    "\n",
    "_, s = GRU2(c)\n",
    "print(\"GRU2 Output shape:\", s.shape)\n",
    "\n",
    "# Eq. (4) Recurrent Skip Component (concatenation)\n",
    "r = torch.cat((h_gru, s.view(bs, -1)), 1)\n",
    "linear1 = nn.Linear(hidden_state_dims_GRU1 + skip*hidden_state_dims_GRU2, <BLANK>)\n",
    "res = linear1(r)\n",
    "print(\"r shape:\", res.shape)\n",
    "\n",
    "# (optional) Temporal Attention Layer (replacing the Recurrent Skip Component)\n",
    "print(\"H_gru shape:\", H_gru.shape)\n",
    "attn_layer = nn.MultiheadAttention(embed_dim=hidden_state_dims_GRU1, num_heads=4, batch_first=True)\n",
    "attn_out, attn_ws = attn_layer(query=H_gru[:, -1:], key=<BLANK>, value=<BLANK>)\n",
    "print(\"attn out shape: \", attn_out.shape)\n",
    "print(\"attn ws shape: \", attn_ws.shape)\n",
    "r2 = torch.cat((h_gru, attn_out.squeeze(1)), 1)\n",
    "\n",
    "linear1_attn = nn.Linear(<BLANK>, out_features)\n",
    "res_attn = linear1_attn(r2)\n",
    "print(\"res attn shape: \", res_attn.shape)\n",
    "\n",
    "# Eq. (5) Autoregressive Component (scaling sensitivity)\n",
    "linear2 = nn.Linear(<BLANK>, 1)\n",
    "z = linear2(inputs.view(<BLANK>, out_features, -1)).squeeze(-1)\n",
    "\n",
    "# Eq. (6) Final result\n",
    "Y_t = res + z \n",
    "Y_t.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## LSTNet Module\n",
    "\n",
    "\n",
    "Now that you’ve worked on a rough LSTNet implementation, let’s integrate it into a PyTorch model. We will focus on the PyTorch framework for now, and later we will use this model within LightningModule.\n",
    "\n",
    "For now, subclass `torch.nn.Module` and define the two essential functions:\n",
    "- `__init__`: For initializing the components.\n",
    "- `forward`: For defining the forward pass of the network.\n",
    "\n",
    "Use the respective components from the previous implementation to complete these methods.\n",
    "\n",
    "Once again, I’ve left certain sections blank for you to fill in. After completing the implementation, **you can compare your solution with mine by using the following magic command**:\n",
    "\n",
    "```python\n",
    "%%load_file \n",
    "../solutions/lstnet_module.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 out_features, \n",
    "                 seq_length, \n",
    "                 num_attn_heads = 4, hidden_state_dims_attn=32,\n",
    "                 n_out_channels=10, window_size=2, hidden_state_dims_GRU1=32, skip=4, \n",
    "                 hidden_state_dims_GRU2=32, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                in_channels=1, \n",
    "                out_channels=n_out_channels, \n",
    "                kernel_size=(window_size, input_dim)\n",
    "            )\n",
    "        \n",
    "        self.GRU1 = nn.GRU(\n",
    "            input_size=n_out_channels,\n",
    "            hidden_size=hidden_state_dims_GRU1,\n",
    "            batch_first=True, \n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.GRU2 = nn.GRU(\n",
    "            input_size=n_out_channels,\n",
    "            hidden_size=hidden_state_dims_GRU2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.skip = skip\n",
    "        self.linear1 = nn.Linear(hidden_state_dims_GRU1 + skip*hidden_state_dims_GRU2, out_features)\n",
    "        self.linear2 = nn.Linear(seq_length, 1)   \n",
    "\n",
    "        self.attn_layer = nn.MultiheadAttention(embed_dim=hidden_state_dims_attn, \n",
    "                                                num_heads=num_attn_heads,\n",
    "                                                dropout=dropout, \n",
    "                                                batch_first=True)     \n",
    "        self.linear1_attn = nn.Linear(hidden_state_dims_attn + hidden_state_dims_GRU1, out_features)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        batch_size = inputs.shape[0] \n",
    "\n",
    "        ## IMPLEMENT THIS\n",
    "\n",
    "        return Y_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Training the model\n",
    "\n",
    "Following the same structure as the previous notebook, we will initialize the LightningModule and Trainer from pytorch_lighnint and let it run the training and validation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesModel(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning module for training the model.\n",
    "    Args:\n",
    "        input_dim (int): Number of time series (8 if there are 8 time series) \n",
    "        out_features (int): number of time series to predict \n",
    "        seq_length (int): number of past time steps given as input\n",
    "        n_out_channels (int): number of kernels in the convolution layer\n",
    "        window_size (int): kernel width in convolution layer\n",
    "        hidden_state_dims_GRU1 (int): dimension of the first recurrent component \n",
    "        skip (int): number of hidden units to skip in skip-recurrent component\n",
    "        hidden_state_dims_GRU2 (int): dimension of the second GRU unit (skip-recurrent component)\n",
    "        num_attn_heads (int): number of attention heads in the attention unit (used only if skip > 0)\n",
    "        hidden_state_dims_attn (int): dimension of attention layer (used only if skip > 0)\n",
    "        dropout (float): probability of dropout (similar to regularization )\n",
    "\n",
    "        learning_rate (float): starting learning rate for adam optimizer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, seq_length, \n",
    "                 num_attn_heads=4, hidden_state_dims_attn=64,\n",
    "                 n_out_channels=16, window_size=2, hidden_state_dims_GRU1=64, skip=4, \n",
    "                 hidden_state_dims_GRU2=64, dropout=0.0, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # can access hyperparams by self.hparams\n",
    "        self.LSTNet = LSTNet(input_dim, output_dim, seq_length, \n",
    "                num_attn_heads, hidden_state_dims_attn, n_out_channels, \n",
    "                window_size, hidden_state_dims_GRU1, skip, hidden_state_dims_GRU2)\n",
    "        \n",
    "        self.loss_function = torch.nn.L1Loss()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.LSTNet(inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch \n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_function(outputs, targets)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch \n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_function(outputs, targets)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# training dataset \n",
    "dataset = utils.SlidingFixedWindow(train_data, seq_length=SEQUENCE_LENGTH)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE,  shuffle=True)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = utils.SlidingFixedWindow(train_val_data[-(SEQUENCE_LENGTH + MAX_FORECASTING_HORIZON): ], SEQUENCE_LENGTH)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,  shuffle=False)\n",
    "\n",
    "model = TimeSeriesModel(input_dim=8, output_dim=8, seq_length=SEQUENCE_LENGTH, skip=0)\n",
    "\n",
    "# define callbacks \n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=True, patience=5)\n",
    "# save the best checkpoint; this will be used to evaluate test metrics\n",
    "best_checkpoint = ModelCheckpoint(save_top_k=1, monitor='val_loss',\n",
    "                                   mode='min', save_on_train_epoch_end=1,\n",
    "                                   filename='{epoch:02d}-{val_loss:.5f}')\n",
    "# save the last checkpoint in case if you want to resume training \n",
    "last_checkpoint =  ModelCheckpoint(save_top_k=1, monitor='step',\n",
    "                                   mode='max', save_on_train_epoch_end=1,\n",
    "                                   filename='{epoch:02d}-{step}')\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20, # preliminary run; we will run hyperparameter tuning in the next step\n",
    "    check_val_every_n_epoch=5, \n",
    "    callbacks=[early_stopping, best_checkpoint, last_checkpoint], \n",
    "    deterministic=True,\n",
    "    )\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the training trajectory**\n",
    "\n",
    "Run the following command on command line and navigate to the port on which tensorboard is launched. Then navigate to `Scalars` tab to see how `train_loss` and `val_loss` changes during the training. \n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=lightning_logs\n",
    "```\n",
    "\n",
    "--- \n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Following the same structure as in the previous notebook, we will use Optuna to perform **hyperparameter optimization** for our LSTNet model. As before, we will define:\n",
    "- An **objective function** that trains the model and returns the performance metric to optimize,\n",
    "- A **search space** for the hyperparameters using `trial.suggest_*` methods,\n",
    "- A **study** to manage the optimization process and evaluate different hyperparameter combinations over several trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [1e-1, 0])\n",
    "    skip = trial.suggest_categorical(\"skip\", [0, 4]) # attention or seasonality\n",
    "\n",
    "    model = TimeSeriesModel(\n",
    "        input_dim=8, output_dim=8, seq_length=SEQUENCE_LENGTH,\n",
    "        skip=skip,\n",
    "        dropout=dropout    \n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=500, \n",
    "        check_val_every_n_epoch=5, \n",
    "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=True, patience=3)], \n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Refit on Train-Val Subset\n",
    "\n",
    "To measure the model's performance on the test data, we will first retrain the model using the combined train-validation dataset. \n",
    "\n",
    "**Note**: We can access the best parameters using `study.best_params`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit on train and val dataset\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "# training dataset \n",
    "dataset = utils.SlidingFixedWindow(train_val_data, seq_length=SEQUENCE_LENGTH)\n",
    "train_val_loader = DataLoader(dataset, batch_size=BATCH_SIZE,  shuffle=True)\n",
    "\n",
    "model = TimeSeriesModel(\n",
    "    input_dim=8, output_dim=8, seq_length=SEQUENCE_LENGTH,\n",
    "    skip=study.best_params['skip'],\n",
    "    dropout=study.best_params['dropout'],\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=500, \n",
    "    callbacks=[EarlyStopping(monitor='train_loss', mode='min', verbose=True, patience=10)], \n",
    "    deterministic=True\n",
    "    )\n",
    "\n",
    "trainer.fit(model, train_val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "##  Forecast\n",
    "\n",
    "Inference in recurrent models needs to be performed iteratively. This is because, during inference, we predict one step at a time. After each prediction, we append the newly predicted value to the input sequence and use it to predict the next time step.\n",
    "\n",
    "This process is repeated iteratively until the required number of future predictions is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the best model\n",
    "model = TimeSeriesModel.load_from_checkpoint(\n",
    "    checkpoint_path=trainer.checkpoint_callback.best_model_path,\n",
    "    input_dim=8, output_dim=8, seq_length=SEQUENCE_LENGTH)\n",
    "model.eval()\n",
    "\n",
    "# evaluate on test set\n",
    "output = torch.tensor([data.iloc[-(SEQUENCE_LENGTH + MAX_FORECASTING_HORIZON):-MAX_FORECASTING_HORIZON].values], \n",
    "                      dtype=torch.float, device=model.device)\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(MAX_FORECASTING_HORIZON):\n",
    "        y = model(output)\n",
    "        output = torch.cat((output, y.unsqueeze(0)), axis=1)\n",
    "        output = output[:, 1:]\n",
    "        y_pred.append(y)\n",
    "\n",
    "y_pred = torch.cat(y_pred)\n",
    "\n",
    "AUGMENTED_COL_NAMES = [f\"{MODEL_NAME}_{col}_mean\" for col in data.columns]\n",
    "test_predictions_df = pd.DataFrame(y_pred.cpu().numpy(), columns=AUGMENTED_COL_NAMES, index=test_data.index)\n",
    "\n",
    "# save them to the directory\n",
    "test_predictions_df.to_csv(f\"{str(RESULTS_DIRECTORY)}/predictions.csv\", index=True)\n",
    "print(test_predictions_df.shape)\n",
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Evaluate \n",
    "\n",
    "Let's compute the metrics by comparing the predictions with that of the target data. Note that we will have to rename the columns of the dataframe to match the expected column names by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalaute metrics\n",
    "target_data = data[-MAX_FORECASTING_HORIZON:]\n",
    "model_metrics, records = utils.get_mase_metrics(\n",
    "    historical_data=train_val_data,\n",
    "    test_predictions=test_predictions_df.rename(\n",
    "            columns={x:x.split(\"_\")[1] for x in test_predictions_df.columns\n",
    "        }),\n",
    "    target_data=target_data,\n",
    "    forecasting_horizons=FORECASTING_HORIZON,\n",
    "    columns=data.columns, \n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "records.to_csv(f\"{str(RESULTS_DIRECTORY)}/metrics.csv\", index=False)\n",
    "records[['col', 'horizon', 'mase']].pivot(index=['horizon'], columns='col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_results(path=DIRECTORY_PATH_TO_SAVE_RESULTS, metric='mase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Plot Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = utils.plot_forecasts(\n",
    "    historical_data=train_val_data,\n",
    "    forecast_directory_path=DIRECTORY_PATH_TO_SAVE_RESULTS,\n",
    "    target_data=target_data,\n",
    "    columns=data.columns,\n",
    "    n_history_to_plot=10, \n",
    "    forecasting_horizon=MAX_FORECASTING_HORIZON,\n",
    "    dpi=200,\n",
    "    exclude_models=['LSTM'],\n",
    "    plot_se=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We explored various deep learning architectures for time series forecasting, particularly by closely following the work of Lai et al. (2018) on LSTNets. Specifically, we leveraged: CNNs to capture local patterns, GRUs to model long-term dependencies, Skip connections or Attention mechanisms to capture very long-term patterns. These approaches combined allow us to model the complex structure of time series data across different time scales.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "- Increase `SEQUENCE_LENGTH` and observe whether this has any impact on model performance.\n",
    "  \n",
    "- Explore ways to include multiple levels of skip connections, i.e., `skip=[4, 12, 24]`.\n",
    "  \n",
    "- Add new features to the inputs, such as day of the week, lag features, or other time-related variables.\n",
    "  \n",
    "- Experiment with different hidden dimensions for the LSTNet to see how the size of the hidden layer affects the model's accuracy.\n",
    "  \n",
    "- Apply a normalization procedure (e.g., **min-max scaling**) to the data, ensuring that only the training data is used for fitting the scaler. Perform the modeling process on the normalized data and, after generating the final model's predictions, invert the normalization to return the output to its original scale. See `sklearn.preprocessing.MinMaxScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))\n",
    "  \n",
    "- Additionally, perform the modeling on the **raw data**, without applying any transformation (such as converting it into log daily returns), to compare results directly with the untransformed dataset.\n",
    "\n",
    "\n",
    "---\n",
    "## Next Steps\n",
    "\n",
    "To learn about more advanced deep learning based approaches, proceed to module 6 (Transformer based models) or module 7 (LLM-based models).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chains1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
