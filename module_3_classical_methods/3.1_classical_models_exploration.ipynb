{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94ce33a-1065-4013-9d7f-26191c986415",
   "metadata": {},
   "source": [
    "# Exchange Rate Dataset\n",
    "\n",
    "**Approximate Learning Time:** Up to 1 hour\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will introduce the **Exchange Rate** dataset, split it into training, validation, and test sets, and explore the training data using the techniques learned in previous modules, thereby setting the stage for the upcoming modeling approaches.\n",
    "\n",
    "---\n",
    "\n",
    "## About Dataset\n",
    "\n",
    "The **exchange rate dataset** contains daily exchange rates from 1990 to 2016 for 8 countries: Australia, Britain, Canada, Switzerland, China, Japan, New Zealand, and Singapore. It includes a total of 8 univariate time series, each with 7,588 time steps. \n",
    "\n",
    "For ease of training in this tutorial series, we will resample the dataset to a weekly frequency, reducing it to a time series with 1,084 time steps.\n",
    "This is done within the `load_tutorial_data` function.\n",
    "\n",
    "**Note:** We will encapsulate these loading and splitting functions into a function in `utils.py` to avoid repetition in every notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7719b0-3d01-4c6c-9be2-5eed1d967a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys; sys.path.append(\"../\")\n",
    "import utils_tfb\n",
    "import utils\n",
    "\n",
    "PLOTTING_COLORS = utils.PLOTTING_COLORS\n",
    "\n",
    "FORECASTING_HORIZON = [4, 8, 12, 24, 52] # weeks \n",
    "MAX_FORECASTING_HORIZON = max(FORECASTING_HORIZON)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e559bd-939f-4463-8bde-04a613b1e5b9",
   "metadata": {},
   "source": [
    "## Load, Transform, and Split Dataset\n",
    "\n",
    "---\n",
    "\n",
    "### Load and Downsample \n",
    "\n",
    "Let's load the dataset and downsample it to weekly frequency. We use `pandas`'s, `resample`([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html)) function to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b23c52-e166-4c82-941d-9e0f344bc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tutotrial_data(dataset):\n",
    "    \"\"\"Loads dataset for tutorial.\"\"\"\n",
    "    TS_DATA_FOLDER = pathlib.Path(\"../forecasting\").resolve()\n",
    "    if dataset == 'exchange_rate':\n",
    "        dataset = TS_DATA_FOLDER / \"Exchange.csv\"\n",
    "        data = utils_tfb.read_data(str(dataset))\n",
    "        data.index.freq = 'D'  # since we know that the frequency is daily\n",
    "        data = data.resample(\"W\").mean() # Resmaple to obtain weekly time series\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized dataset: {dataset}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e48c8-8a71-4147-9001-7af3f4d5dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_tutotrial_data(dataset='exchange_rate')\n",
    "print(\"Sampling frequency of data\", data.index.freq)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7cba7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Transformation \n",
    "\n",
    "Due to the nature of exchange rate, the common practice is to predict the daily returns or log daily returns.\n",
    "Thus, given a time series $\\{x_t\\}_{t=0}^{T} = \\{x_0, x_1, x_2, ..., x_T\\}$, following two transformations are usually considered, \n",
    "\n",
    "1. **Daily Returns** is calculated as \n",
    "\n",
    "$$\n",
    "r_t = \\frac{x_t - x_{t-1}}{x_{t-1}}\n",
    "$$\n",
    "\n",
    "Daily returns has the following properties:\n",
    "- May have skewed distributions with heavy tails.\n",
    "- Can include extreme values or outliers.\n",
    "- Not additive over time.\n",
    "- Distribution is often not normally distributed.\n",
    "\n",
    "\n",
    "2. **Log Daily Returns** is calculated as \n",
    "\n",
    "$$\n",
    "r_t = ln\\big(\\frac{x_t}{x_{t-1}}\\big)\n",
    "$$\n",
    "\n",
    "with the following properties:\n",
    "- Tend to approximate a normal distribution, especially over short intervals.\n",
    "- Additive over time, which is advantageous for cumulative analyses.\n",
    "- Reduce the impact of outliers due to the logarithmic transformation.\n",
    "- Distribution is closer to normality.\n",
    "\n",
    "Using which transformation may be context dependent and might require more expertise to reliably justify. In this notebook, we will perform EDA on the raw data as well as its log transformation. **For our tutorial we will use log daily returns as it follows a nice property of normality which is the assumption behind a lot of modelling approaches.**\n",
    "\n",
    "\n",
    "Let's transform the data to log daily returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of observations before transformation:{data.shape[0]}\")\n",
    "transformed_data = np.log(data / data.shift(1)).dropna()\n",
    "print(f\"Number of observations after transformation:{transformed_data.shape[0]}\")\n",
    "data = data.iloc[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2978ea04",
   "metadata": {},
   "source": [
    "**Exercise**: Why is there a decrement in 1 observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8790935-2faf-4ea2-8428-500bf9af16d0",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "As discussed in previous module, we split the dataset into training, validation, and testing subsets. This splitting is chronological for time series. \n",
    "\n",
    "We are interested in building models that can predict $H$ time steps ahead. For our tutorial series, we will consider various values of H. These are specified in `FORECASTING_HORIZON`. Our forecasting models will be evaluated for all these horizon. Specific choice will be very much dependent on the task at hand. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc6a8d-534a-4ed8-bd56-43cddda37a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data = data.iloc[:-MAX_FORECASTING_HORIZON]\n",
    "train_data, val_data = train_val_data.iloc[:-MAX_FORECASTING_HORIZON], train_val_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "test_data = data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "print(f\"Number of steps in training data: {len(train_data)}\\nNumber of steps in validation data: {len(val_data)}\\nNumber of steps in test data: {len(test_data)}\")\n",
    "\n",
    "transformed_train_val_data = transformed_data.iloc[:-MAX_FORECASTING_HORIZON]\n",
    "transformed_train_data, transformed_val_data = transformed_train_val_data.iloc[:-MAX_FORECASTING_HORIZON], train_val_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "transformed_test_data = transformed_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6565e-dba2-457c-97fa-2badb7de9637",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## EDA of Exchange Rate Dataset\n",
    "\n",
    "**Note:** We will only examine the **training data** to ensure that our choice of modeling techniques is not influenced by the validation or test data, thus preventing bias in the metrics.\n",
    "\n",
    "### Raw Data Visualization\n",
    "\n",
    "Let's plot the raw data as well as its log transformations defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1b725-394f-4f23-9c6c-8b9c8a93e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 8), dpi=100)\n",
    "_ = utils.plot_raw_data(train_data, ax=axs[0])\n",
    "axs[0].set_title(\"Exchange Rates\")\n",
    "_ = utils.plot_raw_data(transformed_train_data, ax=axs[1], cols=data.columns)\n",
    "_= axs[1].set_title(\"Log daily returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85291df-5895-4718-94f6-6ec40a875e32",
   "metadata": {},
   "source": [
    "**Observation**: Although the exchange rate magnitudes vary significantly across countries, the log daily returns tend to fall within a similar range.\n",
    "\n",
    "---\n",
    "\n",
    "### Autocorrelation / Partial Autocorrelation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e6260-36db-489d-b281-06e17ddd3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 8), dpi=100)\n",
    "_ = utils.plot_acf_pacf(train_data,  axs=axs[:, 0], n_lags=50)\n",
    "axs[0, 0].set_title(\"Exchange Rates\\n\\nAuto Correlations\")\n",
    "_ = utils.plot_acf_pacf(transformed_train_data, axs=axs[:, 1], n_lags=10)\n",
    "axs[0, 1].set_title(\"Log daily returns\\n\\nAuto Correlations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1daa233-40ee-4b57-9fc9-56265e7047e4",
   "metadata": {},
   "source": [
    "**Observations**: The raw data shows higher correlations at larger lags, but a sudden decline is observed when using log-transformed data. Partial correlations, which account for intermediate lags, are more in line with the correlations for the log-transformed data. This suggests that a log transformation could result in a better prediction model, potentially reducing the need to include many lag terms.\n",
    "\n",
    "--- \n",
    "\n",
    "### Time Series Decomposition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c98c0e-3da2-49e0-9212-846cc928f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(15, 8), dpi=100)\n",
    "_ = utils.plot_seasonality_decompose(train_data,  axs=axs[:, 0])\n",
    "axs[0, 0].set_title(\"Exchange Rates\\n\\nOriginal\")\n",
    "_ = utils.plot_seasonality_decompose(transformed_train_data, axs=axs[:, 1])\n",
    "_ = axs[0, 1].set_title(\"Log daily returns\\n\\nOriginal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a39be-51cd-4ad8-8aba-8b883540b4c3",
   "metadata": {},
   "source": [
    "**Observations**: \n",
    "\n",
    "- The trend appears relatively stable in both the raw and log-transformed data.\n",
    "  \n",
    "- Seasonality in the log-transformed data is twice the magnitude of the trend, whereas in the raw data, the opposite is true, with trend dominating over seasonality.\n",
    "  \n",
    "- There are sudden peaks in some time series, as revealed by the residuals, which may warrant further investigation into their underlying causes. These anomalies could provide insight into ways to improve the model.\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "### Check Stationarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aab8ef-fbfb-41ba-a981-49cba90d1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_stationarity(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_stationarity(transformed_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293dc7c9-fef1-4c5e-a701-bdec478b2e63",
   "metadata": {},
   "source": [
    "**Observations:** Raw exchange rates exhibit non-stationarity, but the returns are more consistent over time. The log transformation of returns reduces the impact of large price movements and helps stabilize the variance over time, thereby making them appropriate for modeling. \n",
    "\n",
    "\n",
    "**Note**: In the notebooks that follow, we will build forecasting mdoels to predict future log daily returns instead of raw data. \n",
    "\n",
    "--- \n",
    "\n",
    "## Forecasting Strategy\n",
    "\n",
    "In this section, we will discuss various types of forecasting formulations and outline the approach we will use in this tutorial.\n",
    "\n",
    "Broadly, forecasting problems can be categorized into two types:\n",
    "\n",
    "Given a time series $ \\{x_0, x_1, \\dots, x_T\\} $, a forecasting problem may involve either:\n",
    "1. **Single-step forecasting**: Predicting just one step ahead, $ x_{T+1} $.\n",
    "2. **Multi-step forecasting**: Predicting multiple future time steps, say $ H $, i.e., $ \\{\\hat{x}_{T+1}, \\hat{x}_{T+2}, \\dots, \\hat{x}_{T+H}\\} $.\n",
    "\n",
    "**Approaches to Multi-step Forecasting**\n",
    "\n",
    "While single-step forecasting is relatively simple, multi-step forecasting can be approached in several ways:\n",
    "\n",
    "- **Iterative forecasting**: Train a single model to predict the next time step. The model is used iteratively, where the output from one step becomes the input for predicting the next step. This continues until $ H $ steps are forecasted.\n",
    "  \n",
    "- **Direct forecasting**: Train **H different models**, one for each time step in the forecasting horizon, where each model predicts a specific step ahead.\n",
    "\n",
    "- **Hybrid approach**: Train **H different models**, where each model takes in the prediction from the previous model as input, making the models for later steps more informative by incorporating earlier predictions.\n",
    "\n",
    "\n",
    "In **this tutorial**, owing to its simplicity, we will focus on the **iterative approach**, where a single model is trained to predict the next time step, and it is used iteratively to generate multi-step forecasts.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this module, we explored the dataset that will be used throughout the remaining notebooks. We decided on a specific transformation, namely log daily returns. Additionally, we examined several visualizations to form an initial intuition about the data. Finally, we outlined the specifics of the forecasting problem that will be used in the upcoming notebooks.\n",
    "\n",
    "--- \n",
    "\n",
    "## Exercises\n",
    "\n",
    "- Generate plots for the rolling statistics (e.g., mean and standard deviation) of the raw exchange rates and their log daily returns transformation\n",
    "  \n",
    "- Utilize techniques from Exploratory Data Analysis (EDA) for multivariate time series to analyze the log daily returns, identifying similar time series\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Proceed to other notebooks in this module to explore classical forecasting methods on this dataset.\n",
    "- To learn about other machine learning based approaches, check out the module 4 (XGBoost), module 5 (LSTM-based models), module 6 (Transformer based models), or module 7 (LLM-based models).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
