{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bce813-4c66-4678-a7d7-d865f7d3c859",
   "metadata": {},
   "source": [
    "# Classical Forecasting Methods: Vector Autoregression (VAR)\n",
    "\n",
    "\n",
    "**Approximate Learning Time**: Up to 2 hours\n",
    "\n",
    "--- \n",
    "\n",
    "## Vector Autoregression (VAR) \n",
    "\n",
    "\n",
    "As the name suggests, VAR models perform auto-regression on vectors rather than scalars, as in univariate time series. VAR models follow a simple mathematical formulation and are trained using Maximum Likelihood Estimation (MLE) principles. They differ from VARMA models, which involve error terms and are optimized using iterative MLE. Plese refer to this [YouTube video]((https://www.youtube.com/watch?v=0-FKPJ5KxSo)) for a quick and simple introduction to VAR models.\n",
    "\n",
    "For this tutorial, we will focus on VAR models, but you are welcome to explore VARMA models using the `VARMAX` module. Refer to the [documentation](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.varmax.VARMAX.html#statsmodels.tsa.statespace.varmax.VARMAX) for more details. \n",
    "\n",
    "--- \n",
    "\n",
    "Let's load the log daily returns of exchange rates, and split the data into train, validation, and test subsets!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a476e-7956-4ae3-b5bd-17d82f8b91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "import sys; sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "\n",
    "# To avoid flooding of the screen with convergence warnings during hyperparameter tuning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## WARNING: To compare different models on the same horizon, keep this same across the notebooks\n",
    "FORECASTING_HORIZON = [4, 8, 12] # weeks \n",
    "MAX_FORECASTING_HORIZON = max(FORECASTING_HORIZON)\n",
    "\n",
    "SEQUENCE_LENGTH = 2 * MAX_FORECASTING_HORIZON\n",
    "PREDICTION_LENGTH = MAX_FORECASTING_HORIZON\n",
    "\n",
    "DIRECTORY_PATH_TO_SAVE_RESULTS = pathlib.Path('../results/DIY/').resolve()\n",
    "MODEL_NAME = \"VAR\"\n",
    "\n",
    "RESULTS_DIRECTORY = DIRECTORY_PATH_TO_SAVE_RESULTS / MODEL_NAME\n",
    "if RESULTS_DIRECTORY.exists():\n",
    "    print(colored(f'Directory {str(RESULTS_DIRECTORY)} already exists.'\n",
    "           '\\nThis notebook will overwrite results in the same directory.'\n",
    "           '\\nYou can also create a new directory if you want to keep this directory untouched.'\n",
    "           ' Just change the `MODEL_NAME` in this notebook.\\n', \"red\" ))\n",
    "else:\n",
    "    RESULTS_DIRECTORY.mkdir(parents=True)\n",
    "\n",
    "data, transformed_data = utils.load_tutotrial_data(dataset='exchange_rate', log_transform=True)\n",
    "data = transformed_data \n",
    "\n",
    "train_val_data = data.iloc[:-MAX_FORECASTING_HORIZON]\n",
    "train_data, val_data = train_val_data.iloc[:-MAX_FORECASTING_HORIZON], train_val_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "test_data = data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "\n",
    "print(f\"Number of steps in training data: {len(train_data)}\\nNumber of steps in validation data: {len(val_data)}\\nNumber of steps in test data: {len(test_data)}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195f5fd-cde6-4327-90d2-b65cfa6b0366",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## VAR / VARMAX Model\n",
    "\n",
    "In this section, we will fit VAR and VARMAX models to the training data. \n",
    "\n",
    "We use the `VAR`([documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.vector_ar.var_model.VAR.html)) model from `statsmodels`. VAR models accept the parameter `max_lags` and chose the best autoregressive order accordingly. \n",
    "\n",
    "To find the best VARMA model, we will be using `VARMAX` ([documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.varmax.VARMAX.html)) from `statsmodels`. In contrast to VAR model, we need to specify the order. As a result, we will run a small hyperparameter search on the order. We will pick the one that has the best overall performance.\n",
    "\n",
    "**Note**: We will chose the parameters based on the best mean mase across the time series.\n",
    "\n",
    "Let's define a function that builds both the models, and call this function with different parameters to find the best parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be074194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_forecast_VARMA(data, forecast_horizon, p=10, q=5):\n",
    "    \"\"\"\n",
    "    Trains VARMA or VAR model depending on the parameters passed.\n",
    "\n",
    "    Args:\n",
    "        data: pandas DataFrame\n",
    "        forecast_horizon: number fo steps to forecast\n",
    "        p: autoregressive order\n",
    "        q: moving average order; if q=0, p is assumed to be max_lags parameter of VAR Model\n",
    "    \"\"\"\n",
    "    if q == 0:\n",
    "        print(f\"Training a VAR model; Selecting the best autoregressive order from p={{1, 2, ..., {p}}}\")\n",
    "        model = VAR(data).fit(maxlags=p, ic='aic') # note it will automatically select the best order for AR\n",
    "\n",
    "        # make forecasts\n",
    "        lag_order = model.k_ar # best chosen order\n",
    "        print(f\"Best p: {lag_order}\")\n",
    "        forecasted_values = model.forecast(data.values[-lag_order:], steps=forecast_horizon)\n",
    "    else:\n",
    "        print(f\"Fitting VARMAX for parameters: p={p}\\tq={q}\\n\")\n",
    "        model = VARMAX(data, order=(p, q)).fit(maxiter=50, disp=False)\n",
    "        forecasted_values = model.forecast(steps=MAX_FORECASTING_HORIZON) # make predictions \n",
    "\n",
    "    date_range = pd.date_range(start=data.index[-1], periods=forecast_horizon+1, freq=data.index.freq)\n",
    "    date_range = date_range[1:] # we don't need the last index of train\n",
    "\n",
    "    forecasted_df = pd.DataFrame(forecasted_values, index=date_range, columns=train_data.columns)\n",
    "    \n",
    "    return model, forecasted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ee2b2",
   "metadata": {},
   "source": [
    "Let's find the best VAR model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mase = {}\n",
    "\n",
    "# train a VAR model\n",
    "max_p = 10\n",
    "model, forecasted_df = train_and_forecast_VARMA(train_data, forecast_horizon=MAX_FORECASTING_HORIZON, p=max_p, q=0)\n",
    "\n",
    "# compute mase\n",
    "all_mase_simple_var = utils.get_mase(forecasted_df, val_data, train_data)\n",
    "model_mase[(max_p, )] = np.mean(list(all_mase_simple_var.values()))\n",
    "print(f\"MASE: {model_mase[(max_p, )]: 0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fdb4a",
   "metadata": {},
   "source": [
    "Let's find the best VARMA model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best VARMA model\n",
    "p_values = [1, 2]\n",
    "q_values = [1, 2]\n",
    "pq_values = list(itertools.product(p_values, q_values))\n",
    "print(f\"Searching for best (p, q) among {len(pq_values)} combinations in VARMAX model\")\n",
    "for p,q in tqdm(pq_values):\n",
    "    model, forecasted_df = train_and_forecast_VARMA(train_data, forecast_horizon=MAX_FORECASTING_HORIZON, p=p, q=q)\n",
    "\n",
    "    # compute mase\n",
    "    x = utils.get_mase(forecasted_df, val_data, train_data)\n",
    "    model_mase[(p, q)] = np.mean(list(x.values()))\n",
    "    print(f\"MASE: {model_mase[(p, q)]: 0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e928511-8deb-441f-b712-3cb9c2e1acbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Refit on Train-Val Subset & Forecast\n",
    "\n",
    "To measure the model's performance on the test data, we will first retrain the model using the combined train-validation dataset. Then, we will compute the MASE metric on the test dataset to evaluate its performance.\n",
    "Additionally, we will store the test predictions for later comparison with other forecasting methods.\n",
    "\n",
    "We observe that the simple VAR model is better so we use that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ace9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = [key for key in model_mase.keys() if model_mase[key] == min(model_mase.values())][0]\n",
    "p = best_params[0]\n",
    "q = 0 if len(best_params) == 1 else best_params[1]\n",
    "print(f\"Best parameters: p={p} q={q}\")\n",
    "model, forecasted_df = train_and_forecast_VARMA(train_val_data, forecast_horizon=MAX_FORECASTING_HORIZON, p=p, q=q)\n",
    "\n",
    "AUGMENTED_COL_NAMES = [f\"{MODEL_NAME}_{col}_mean\" for col in data.columns]\n",
    "test_predictions_df = pd.DataFrame(forecasted_df.values, columns=AUGMENTED_COL_NAMES, index=test_data.index)\n",
    "test_predictions_df.to_csv(f\"{str(RESULTS_DIRECTORY)}/predictions.csv\", index=True)\n",
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55739d08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate\n",
    "\n",
    "Let's compute the metrics by comparing the predictions with that of the target data. Note that we will have to rename the columns of the dataframe to match the expected column names by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00c843-a179-476f-a7b7-2df082460576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MASE metrics\n",
    "model_metrics, records = utils.get_mase_metrics(\n",
    "    historical_data=train_val_data,\n",
    "    test_predictions=test_predictions_df.rename(\n",
    "            columns={x:x.split(\"_\")[1] for x in test_predictions_df.columns\n",
    "        }),\n",
    "    target_data=test_data,\n",
    "    forecasting_horizons=FORECASTING_HORIZON,\n",
    "    columns=data.columns, \n",
    "    model_name=MODEL_NAME,\n",
    ")\n",
    "\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "records.to_csv(f\"{str(RESULTS_DIRECTORY)}/metrics.csv\", index=False)\n",
    "records[['col', 'horizon', 'mase']].pivot(index=['horizon'], columns='col')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdb664",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12780097",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_results(path=DIRECTORY_PATH_TO_SAVE_RESULTS, metric='mase')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaebe8a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Plot Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e28574-b269-4572-84c6-3278958a654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = utils.plot_forecasts(\n",
    "    historical_data=train_val_data,\n",
    "    forecast_directory_path=DIRECTORY_PATH_TO_SAVE_RESULTS,\n",
    "    target_data=test_data,\n",
    "    columns=data.columns,\n",
    "    n_history_to_plot=10, \n",
    "    forecasting_horizon=MAX_FORECASTING_HORIZON,\n",
    "    dpi=200,\n",
    "    plot_se=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201baa6-fcc3-49b1-8afa-2ebf2d61a742",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Conclusions\n",
    "\n",
    "We explored classical multivariate time series forecasting methods, specifically **VAR** (Vector Autoregression) and **VARMAX** (Vector Autoregressive Moving Average with Exogenous Variables), as generalizations of ARIMA for multivariate time series. We trained these models with various parameter configurations and evaluated the performance of the best model.\n",
    "\n",
    "--- \n",
    "\n",
    "## Exercises\n",
    "\n",
    "- Check the effect of hyperparameter `trend` on model performances above\n",
    "\n",
    "- Apply a normalization procedure (e.g., **min-max scaling**) to the data, ensuring that only the training data is used for fitting the scaler. Perform the modeling process on the normalized data and, after generating the final model's predictions, invert the normalization to return the output to its original scale. See `sklearn.preprocessing.MinMaxScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))\n",
    "\n",
    "- Additionally, perform the modeling on the **raw data**, without applying any transformation (such as converting it into log daily returns), to compare results directly with the untransformed dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To learn about machine learning based approaches, check out the module 4 (XGBoost), module 5 (LSTM-based models), module 6 (Transformer-based models), or module 7 (LLM-based models).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
