{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methods for Forecasting\n",
    "\n",
    "**Approximate Learning Time**:Up to 4 hours\n",
    "\n",
    "--- \n",
    "\n",
    "In this module, we will explore **deep learning architectures** for time series forecasting. Specifically:\n",
    "\n",
    "- We will cover **Multi-Layer Perceptrons (MLP)**, **Recurrent Neural Networks (RNN)**, and **Long-Short Term Memory (LSTM)** architectures in this notebook.\n",
    "- In the next notebook, we will implement **LSTNet**, a model proposed by Lai et al. (2018). This will expose you to the challenges and considerations in applying LSTM to time series forecasting, as well as strategies for addressing these issues.\n",
    "\n",
    "Throughout this module, we will use the **PyTorch** and **PyTorch Lightning** frameworks to facilitate model training and experimentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Overview of Deep Learning Models\n",
    "\n",
    "<ins>**What is Deep Learning?**<ins>\n",
    "\n",
    "\n",
    "Deep learning is a subset of machine learning that focuses on using **neural networks** with many layers (hence \"deep\"). These networks are designed to automatically learn and extract meaningful patterns from large datasets. Deep learning excels in tasks such as image recognition, natural language processing, and time series forecasting, where traditional models may struggle to capture complex relationships.\n",
    "\n",
    "<ins>**What is MLP?**</ins>\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** is a type of **feedforward neural network** consisting of multiple layers of neurons:\n",
    "- An input layer,\n",
    "- One or more hidden layers,\n",
    "- An output layer.\n",
    "\n",
    "Each neuron is connected to every neuron in the next layer, and the network learns by adjusting the weights of these connections to minimize the error between predicted and actual outputs. MLPs are good for tasks where patterns can be learned directly from the input data, but they struggle with sequential data like time series.\n",
    "\n",
    "<ins>**What is RNN?**</ins>\n",
    "\n",
    "A **Recurrent Neural Network (RNN)** is a type of neural network designed to handle **sequential data**, such as time series or text. Unlike MLPs, RNNs have connections that loop back on themselves, allowing them to \"remember\" information from previous steps in the sequence. This makes RNNs well-suited for tasks where context or memory is important.\n",
    "\n",
    "However, RNNs often face issues like **vanishing gradients**, which can make learning long-term dependencies difficult.\n",
    "\n",
    "<ins>**What is LSTM?**</ins>\n",
    "\n",
    "**Long-Short Term Memory (LSTM)** is a type of RNN that addresses the limitations of traditional RNNs, particularly the issue of vanishing gradients. LSTMs introduce special units called **gates** (input, forget, and output gates) that allow the network to selectively retain or discard information over long sequences. This enables LSTMs to learn both **short-term** and **long-term** dependencies more effectively, making them ideal for time series forecasting where patterns span over different timescales.\n",
    "\n",
    "\n",
    "Please refer to this [excellent blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) to understand more about LSTMs and RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "In this session, we will focus on building forecasting models using **LSTM networks**. While we will primarily work with LSTMs, replacing them with **RNNs** or **MLPs** is relatively straightforward and will be left as an exercise for you to explore.\n",
    "\n",
    "**Note**: \n",
    "There are several Python libraries available that offer pre-built implementations for time series forecasting. However, as of September 2024, many of these libraries are still under active development. While they can be useful due to their pre-specified knowledge and tools, it’s valuable to learn how to implement your own models from scratch. This will give you a deeper understanding of the underlying mechanics. That being said, here are some libraries you might want to be aware of:\n",
    "\n",
    "- [**GluonTS**](https://ts.gluon.ai/stable/index.html): A comprehensive library offering several [models](https://ts.gluon.ai/stable/getting_started/models.html) widely used in the academic community.\n",
    "- [**Pytorch Forecasting**](https://pytorch-forecasting.readthedocs.io/en/stable/index.html): A popular library built on top of PyTorch, designed specifically for time series forecasting.\n",
    "- [**Neuralforecast**](https://github.com/Nixtla/neuralforecast): Provides several deep learning-based forecasting algorithms.\n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "[(Lai et al., 2018) Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks](https://arxiv.org/abs/1703.07015)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Let's load the log daily returns of exchange rates, and split the data into train, validation, and test subsets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning as L \n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import optuna\n",
    "\n",
    "## WARNING: To compare different models on the same horizon, keep this same across the notebooks\n",
    "from termcolor import colored\n",
    "import sys; sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "FORECASTING_HORIZON = [4, 8, 12] # weeks \n",
    "MAX_FORECASTING_HORIZON = max(FORECASTING_HORIZON)\n",
    "\n",
    "SEQUENCE_LENGTH = 2 * MAX_FORECASTING_HORIZON\n",
    "PREDICTION_LENGTH = MAX_FORECASTING_HORIZON\n",
    "\n",
    "DIRECTORY_PATH_TO_SAVE_RESULTS = pathlib.Path('../results/DIY/').resolve()\n",
    "MODEL_NAME = \"LSTM\"\n",
    "\n",
    "RESULTS_DIRECTORY = DIRECTORY_PATH_TO_SAVE_RESULTS / MODEL_NAME\n",
    "if RESULTS_DIRECTORY.exists():\n",
    "    print(colored(f'Directory {str(RESULTS_DIRECTORY)} already exists.'\n",
    "           '\\nThis notebook will overwrite results in the same directory.'\n",
    "           '\\nYou can also create a new directory if you want to keep this directory untouched.'\n",
    "           ' Just change the `MODEL_NAME` in this notebook.\\n', \"red\" ))\n",
    "else:\n",
    "    RESULTS_DIRECTORY.mkdir(parents=True)\n",
    "\n",
    "data, transformed_data = utils.load_tutotrial_data(dataset='exchange_rate', log_transform=True)\n",
    "data = transformed_data\n",
    "\n",
    "train_val_data = data.iloc[:-MAX_FORECASTING_HORIZON]\n",
    "train_data, val_data = train_val_data.iloc[:-MAX_FORECASTING_HORIZON], train_val_data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "test_data = data.iloc[-MAX_FORECASTING_HORIZON:]\n",
    "print(f\"Number of steps in training data: {len(train_data)}\\nNumber of steps in validation data: {len(val_data)}\\nNumber of steps in test data: {len(test_data)}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Long Short-Term Memory (LSTM)\n",
    "\n",
    "PyTorch provides LSTM functionality through the `nn.LSTM` class ([documentation here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)). The two main arguments required for setting up an LSTM are: (a) **Input size**: The number of features associated with each step in the sequence, (b) **Hidden size**: The size of the internal hidden state representation in the LSTM.\n",
    "\n",
    "Neural networks learn by adjusting their weights according to an **objective function** (or loss function). These weight updates are made using the gradients of the objective function, often calculated on a batch of samples. Therefore, during training, data is processed in **batches**.\n",
    "\n",
    "We will use the convention `batch_first=True`, which means our input tensors will have the following shape:`(batch size, number of time steps in the sequence, dimension of each step)`.\n",
    "\n",
    "In time series data, each time step can be featurized in different ways. While many time series models use a single feature per time step, it is possible to include multiple features, such as lagged values or categorical features like day of the week, making the dimension of each time step larger than one.\n",
    "\n",
    "Additionally, time series data may have sequences of varying lengths. In cases where some sequences are shorter than others, **padding** is applied. Padding involves a separate tensor that indicates where the valid data is, using 1s for valid data points and 0s for the padded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=12\n",
    "seq_length = 10\n",
    "input_dim = 8\n",
    "hidden_dim = 16\n",
    "input = torch.randn(batch_size, seq_length, input_dim)\n",
    "\n",
    "lstm = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "out, (h, _) = lstm(input)\n",
    "print(\"Shape of output: \", out.shape)\n",
    "print(\"Shape of the final hidden state: \", h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Dataloading for model training\n",
    "\n",
    "As mentioned earlier, deep learning frameworks require special handling of data, particularly when it comes to **batching**. Batching is crucial for computational efficiency, as it allows models to process multiple samples simultaneously, leveraging hardware like GPUs to speed up training.\n",
    "\n",
    "\n",
    "PyTorch provides two essential components for working with data:\n",
    "\n",
    "- `torch.utils.data.Dataset`: This class defines how to retrieve a single data sample from a dataset. It's designed to abstract data access, allowing you to focus on what a sample looks like, not how it’s retrieved.\n",
    "- `torch.utils.data.DataLoader`: This takes an instance of Dataset and efficiently loads samples in parallel, using multiple CPUs to batch the data, making it ready for training or inference.\n",
    "\n",
    "In PyTorch, creating a custom dataset involves subclassing `torch.utils.data.Dataset` and defining at least two key methods:\n",
    "\n",
    "- `__len__`: This method returns the number of samples in your dataset. It defines the range of valid indices that can be passed to `__getitem__`.\n",
    "- `__getitem__`: This method defines how to fetch a single data sample, given its index. Each call to `__getitem__` should return the data point associated with the provided index.\n",
    "\n",
    "By implementing these two methods, we can effectively define how data is accessed from any underlying data source—whether it's a CSV file, a database, or a time series dataset like in our case.\n",
    "\n",
    "---\n",
    "\n",
    "We will implement a **sliding fixed-window approach** for fetching data. To keep things simple for now, we will not handle sequences that are shorter than the required sequence length. This will be left as an exercise for you to explore. In the next module, we will introduce **GluonTS**, which automatically handles varying sequence lengths.\n",
    "\n",
    "In this approach, we aim to create windows of data where the indices go from 0 to `len(data) - seq_length`. The target for each window is the value of the next step after the window.\n",
    "\n",
    "The class below implements the sliding window logic, and the following cells will initialize this class with both the training and validation datasets. Note that for the validation dataset, the targets will come exclusively from the validation data, while the inputs may still include data from the training set to maintain the sequential nature of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training series\n",
    "class SlidingFixedWindow(Dataset):\n",
    "    \"\"\"\n",
    "    Returns a time series data iterator with sliding window of fixed size.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas dataframe \n",
    "        seq_length (int): number of past time steps to include in the training sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, seq_length=100):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.data[index:index+self.seq_length].values, dtype=torch.float),\n",
    "            torch.tensor(self.data.iloc[index+self.seq_length].values, dtype=torch.float)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example run to check the batch sizes\n",
    "dataset = SlidingFixedWindow(train_data, SEQUENCE_LENGTH)\n",
    "train_loader = DataLoader(dataset, batch_size=12,  shuffle=True)\n",
    "\n",
    "print(\"Training Dataset (showing only 1 batch)\")\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch \n",
    "    print(f\"SlidingFixedWindow:\\tShape of inputs: {inputs.shape}\\tShape of targets: {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we keep only the last few steps for validation purposes, we truncate train_val_data\n",
    "print(\"Validation Dataset\")\n",
    "val_dataset = SlidingFixedWindow(train_val_data[-(SEQUENCE_LENGTH+MAX_FORECASTING_HORIZON):], SEQUENCE_LENGTH)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10,  shuffle=False)\n",
    "for batch in val_loader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"SlidingFixedWindow:\\tShape of inputs: {inputs.shape}\\tShape of targets: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Lightning: Training the model\n",
    "\n",
    "\n",
    "PyTorch provides a flexible interface for implementing deep learning models, but a lot of the training code tends to look quite similar across different projects. Many codebases include best practices and optimizations that help streamline the training process.\n",
    "\n",
    "**PyTorch Lightning** is a framework designed to simplify the training of neural networks by abstracting much of the boilerplate code involved in training. It does this through the `LightningModule`, a class that you subclass to define the core components of your model.\n",
    "\n",
    "In a `LightningModule`, you’ll need to implement the following key functions:\n",
    "- `forward`: Defines the forward pass of the network.\n",
    "- `training_step`: Specifies the logic for a single training step.\n",
    "- `validation_step` (optional): Defines the validation logic for a single validation step.\n",
    "- `configure_optimizers`: Sets up the optimizer(s) and learning rate scheduler(s) for training.\n",
    "\n",
    "Internally, PyTorch Lightning handles the standard training loops by calling these methods. The framework also provides a `Trainer` class that manages the execution of the training process. `Trainer` comes with a variety of arguments and options to customize training, many of which we will explore in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesModel(L.LightningModule):\n",
    "    def __init__(self, input_dim=1, hidden_dim=150, output_dim=1, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # can access hyperparams by self.hparams\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return self.fc(hidden[-1])\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch \n",
    "        outputs = self(inputs)\n",
    "        loss = torch.nn.L1Loss()(outputs, targets)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch \n",
    "        outputs = self(inputs)\n",
    "        loss = torch.nn.L1Loss()(outputs, targets)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now train the model for **20 epochs**, where each epoch consists of a set number of batches. This number is typically determined by the total length of the dataset, once it has been split into batches.\n",
    "\n",
    "The argument `check_val_every_epoch` specifies how frequently the validation step is run. In our case, we set it to run the `validation_step` every 5 epochs, which is when we will see the `val_loss` being logged.\n",
    "\n",
    "**PyTorch Lightning** also supports a wide range of **callbacks** ([documentation](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html)) that can be passed to the `Trainer` to enhance training flexibility. For example, we are using the **EarlyStopping** callback ([documentation](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping)), which stops the training if the monitored metric (e.g., validation loss) does not improve for a specified number of epochs, defined by the `patience` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42, workers=True)\n",
    "BATCH_SIZE=64\n",
    "\n",
    "# training dataset \n",
    "dataset = SlidingFixedWindow(train_data, seq_length=SEQUENCE_LENGTH)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE,  shuffle=True)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = SlidingFixedWindow(train_val_data[-(SEQUENCE_LENGTH + MAX_FORECASTING_HORIZON): ], SEQUENCE_LENGTH)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,  shuffle=False)\n",
    "\n",
    "model = TimeSeriesModel(input_dim=train_val_data.shape[1], output_dim=train_val_data.shape[1])\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20, # preliminary run; we run hyperparameter tuning in the next step\n",
    "    check_val_every_n_epoch=5, \n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=True, patience=5)], \n",
    "    deterministic=True\n",
    "    )\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've just trained our first model.\n",
    "\n",
    "Now, let's examine how the **loss** evolved throughout the training process. To do this, we'll use **TensorBoard**, which is integrated into the Lightning framework. If you haven't installed TensorBoard yet, you can do so from its [homepage](https://pypi.org/project/tensorboard/).\n",
    "\n",
    "Once TensorBoard is installed, run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=lightning_logs\n",
    "```\n",
    "\n",
    "After launching TensorBoard, navigate to the port it specifies in your browser. In the **Scalars** tab, you can observe how `train_loss` and `val_loss` changed during the training process. Note that the run name is displayed next to `v_num` in the progress bar during training, so you can easily identify your session.\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "As you may notice, there are hyperparameters that we hard-coded in the previous section, such as `learning_rate` and `hidden_dim`. These hyperparameters significantly impact the model’s performance. To optimize these values, we will conduct a **hyperparameter search** using **Optuna** ([documentation](https://optuna.readthedocs.io/en/stable/)).\n",
    "\n",
    "Using Optuna is fairly straightforward and requires four key components:\n",
    "\n",
    "1. **Objective function**: The function that Optuna tries to optimize (e.g., minimizing validation loss).\n",
    "2. **Trial**: Each trial is a single evaluation of the objective function with a specific set of hyperparameters.\n",
    "3. **Search space**: This defines the range of values Optuna will explore for each hyperparameter. We define the search space using `Optuna.trial`.\n",
    "4. **Study**: We create an `optuna.Study` object with the objective function, which tries to optimize the hyperparameters using internal optimization algorithms.\n",
    "\n",
    "Typically, **learning rate** is searched on a **log-uniform scale** because it often takes smaller values across many orders of magnitude. However, for simplicity and because we will only run a few trials in this tutorial, we will use a categorical search for the learning rate.\n",
    "\n",
    "We will also search for the best **hidden dimension** (`hidden_dim`) from the values `[256, 512]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    # typical usage\n",
    "    # learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1)\n",
    "    # hidden_dim = trial.suggest_int(\"hidden_dim\", 128, 256)\n",
    "\n",
    "    # for tutorial we go light\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-3, 1e-2])\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [256, 512])\n",
    "\n",
    "    model = TimeSeriesModel(\n",
    "        input_dim=train_val_data.shape[1], \n",
    "        output_dim=train_val_data.shape[1],\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_dim=hidden_dim    \n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=200, \n",
    "        check_val_every_n_epoch=5, \n",
    "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=True, patience=3)], \n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Refit on Train-Val Subset\n",
    "\n",
    "To measure the model's performance on the test data, we will first retrain the model using the combined train-validation dataset. \n",
    "\n",
    "**Note**: We can access the best parameters using `study.best_params`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit on train and val dataset\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "# training dataset \n",
    "dataset = SlidingFixedWindow(train_val_data, seq_length=SEQUENCE_LENGTH)\n",
    "train_val_loader = DataLoader(dataset, batch_size=BATCH_SIZE,  shuffle=True)\n",
    "\n",
    "model = TimeSeriesModel(\n",
    "    input_dim=train_val_data.shape[1], \n",
    "    output_dim=train_val_data.shape[1],\n",
    "    hidden_dim=study.best_params['hidden_dim'],\n",
    "    learning_rate=study.best_params['learning_rate'],\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=200, \n",
    "    callbacks=[EarlyStopping(monitor='train_loss', mode='min', verbose=False, patience=10)], \n",
    "    deterministic=True\n",
    "    )\n",
    "\n",
    "trainer.fit(model, train_val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "##  Forecast\n",
    "\n",
    "Inference in recurrent models needs to be performed iteratively. This is because, during inference, we predict one step at a time. After each prediction, we append the newly predicted value to the input sequence and use it to predict the next time step.\n",
    "\n",
    "This process is repeated iteratively until the required number of future predictions is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "output = torch.tensor([data.iloc[-(SEQUENCE_LENGTH + MAX_FORECASTING_HORIZON):-MAX_FORECASTING_HORIZON].values], dtype=torch.float)\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(MAX_FORECASTING_HORIZON):\n",
    "        y = model(output)\n",
    "        output = torch.cat((output, y.unsqueeze(0)), axis=1)\n",
    "        output = output[:, 1:]\n",
    "        y_pred.append(y)\n",
    "\n",
    "y_pred = torch.cat(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTED_COL_NAMES = [f\"{MODEL_NAME}_{col}_mean\" for col in data.columns]\n",
    "test_predictions_df = pd.DataFrame(y_pred.numpy(), columns=AUGMENTED_COL_NAMES, index=test_data.index)\n",
    "test_predictions_df.to_csv(f\"{str(RESULTS_DIRECTORY)}/predictions.csv\", index=True)\n",
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Evaluate \n",
    "\n",
    "Let's compute the metrics by comparing the predictions with that of the target data. Note that we will have to rename the columns of the dataframe to match the expected column names by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = data[-MAX_FORECASTING_HORIZON:]\n",
    "model_metrics, records = utils.get_mase_metrics(\n",
    "    historical_data=train_val_data,\n",
    "    test_predictions=test_predictions_df.rename(\n",
    "            columns={x:x.split(\"_\")[1] for x in test_predictions_df.columns\n",
    "        }),\n",
    "    target_data=target_data,\n",
    "    forecasting_horizons=FORECASTING_HORIZON,\n",
    "    columns=data.columns, \n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "records = pd.DataFrame(records)\n",
    "\n",
    "records.to_csv(f\"{str(RESULTS_DIRECTORY)}/metrics.csv\", index=False)\n",
    "records[['col', 'horizon', 'mase']].pivot(index=['horizon'], columns='col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_results(path=DIRECTORY_PATH_TO_SAVE_RESULTS, metric='mase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Plot Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = utils.plot_forecasts(\n",
    "    historical_data=train_val_data,\n",
    "    forecast_directory_path=DIRECTORY_PATH_TO_SAVE_RESULTS,\n",
    "    target_data=test_data,\n",
    "    columns=data.columns,\n",
    "    n_history_to_plot=10, \n",
    "    forecasting_horizon=MAX_FORECASTING_HORIZON,\n",
    "    dpi=200, \n",
    "    plot_se=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Expanding Window DataLoader\n",
    "\n",
    "\n",
    "In this optional section, we will look at the design of an expanding window dataloader. \n",
    "We want to be able to handle sequences of varying lengths.\n",
    "The difficulty arises because `torch.nn.utils.DataLoader` batches all the samples in a single tensor and a tensor can't have variable length elements.\n",
    "As a result, we need to pad our sequences so that all of them are of the same length. \n",
    "\n",
    "`collate_fn` argument passed to `torch.utils.data.DataLoader`, helps in preprocessing individual samples returned from `torch.utils.data.Dataset` so that they are of the same length.\n",
    "PyTorch provides efficient implmenetation of recurrent networks such as RNNs, LSTMs while handling sequences of unequal lengths.\n",
    "This implmementation requires `PackedSequence` object, which is a batch of unequal sequences.\n",
    "\n",
    "`PackedSequence` has two attributes: \n",
    "  - **Data**: A flattened list of all the non-padded elements from the input sequences, stacked together.\n",
    "  - **Batch sizes**: A tensor that specifies how many valid (non-padded) sequences are present at each time step.\n",
    "\n",
    "\n",
    "Thus, a value of 12 at index 1 of `batch_sizes` mean that there are 12 sequences that are active at that time step. \n",
    "Recurrent networks in PyTorch are designed to efficiently process the inputs by keeping track of the number of seqeunces alive at any time step, thereby not wasting compute resources on paddings. \n",
    "\n",
    "\n",
    " - By using the **`batch_sizes`** tensor, PyTorch knows exactly how many sequences are \"alive\" (non-padded) at each time step.\n",
    " - Instead of iterating over all time steps for all sequences (which would involve unnecessary computations on padded values), PyTorch processes only the non-padded entries.\n",
    " \n",
    " In essence, PyTorch only processes the valid data points in the `PackedSequence`, while completely ignoring any padded elements.\n",
    "\n",
    "\n",
    "The `batch_sizes` attribute of this object holds the number of sequences alive at a particular index. \n",
    "Thus, a value of 12 at index 1 of `batch_sizes` mean that there are 12 sequences that are active at that time step. \n",
    "Recurrent networks in PyTorch are designed to efficiently process the inputs by keeping track of the number of seqeunces alive at any time step, thereby not wasting compute resources on paddings. \n",
    "\n",
    "PyTorch’s `PackedSequence` helps avoid processing padding in recurrent neural networks (RNNs, LSTMs, GRUs) by skipping the padded values during computation. \n",
    "\n",
    "\n",
    "Let's see how this works in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "class ExpandingWindow(Dataset):\n",
    "    \"\"\"\n",
    "    Returns a expanding window data iterator for time series.\n",
    "    Args:\n",
    "        min_index (int): defines the minimum number of past time steps to include in the training sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, min_index=0):\n",
    "        self.data = data\n",
    "        self.min_index = min_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.min_index - 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        current_index = index + self.min_index\n",
    "        return (\n",
    "            # +1 so that at index 0 we don't get an empty batch\n",
    "            torch.tensor(self.data[:current_index + 1].values, dtype=torch.float), \n",
    "            torch.tensor(self.data.iloc[current_index + 1].values, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    windows, targets = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor([len(window) for window in windows])\n",
    "    # print(\"lengths\\t \", lengths)\n",
    "\n",
    "    padded_windows = pad_sequence(windows, batch_first=True)\n",
    "    # print(\"padded_windows shape\\t\", padded_windows.shape)\n",
    "\n",
    "    packed_windows = pack_padded_sequence(padded_windows, lengths, batch_first=True, enforce_sorted=False)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return packed_windows, targets\n",
    "\n",
    "\n",
    "dataset = ExpandingWindow(train_data, min_index=100)\n",
    "train_loader = DataLoader(dataset, batch_size=12,  collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "print(\"Training dataset (showing only 1 sample)\")\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch \n",
    "    print(f\"ExpandingWindow:\\nPackedWindow data shape:{inputs.data.shape}\\t PackedWindow batch_sizes shape:{inputs.batch_sizes.shape}\\tShape of targets: {targets.shape}\")\n",
    "    print(f\"Sum of batch_sizes in PackedWindow: {inputs.batch_sizes.sum()}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ExpandingWindow(train_val_data, min_index=len(train_data)-1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10,  collate_fn=collate_fn, shuffle=False)\n",
    "print(\"validaton dataset\")\n",
    "for batch in val_loader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"ExpandingWindow:\\nPackedWindow data shape:{inputs.data.shape}\\t PackedWindow batch_sizes shape:{inputs.batch_sizes.shape}\\tShape of targets: {targets.shape}\")\n",
    "    print(f\"Sum of batch_sizes in PackedWindow: {inputs.batch_sizes.sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We covered how to build deep learning models for time series forecasting. We learned how to implement **dataloaders** in PyTorch to efficiently pack and prepare samples for time series forecasting. Additionally, we explored how to use **PyTorch** and the **Lightning framework** to build and train LSTM models, simplifying the model training process and improving efficiency. Finally, we learned how to use **Optuna** for hyperparameter optimization.\n",
    "\n",
    "---\n",
    "## Exercises\n",
    "\n",
    "- Increase `SEQUENCE_LENGTH` and observe whether this has any impact on model performance.\n",
    "\n",
    "- Adapt the SlidingFixedWindow method to handle **variable-length sequences** in the input data.\n",
    "  \n",
    "- Add new features to the inputs, such as day of the week, lag features, or other time-related variables.\n",
    "  \n",
    "- Experiment with different hidden dimensions for the LSTM to see how the size of the hidden layer affects the model's accuracy.\n",
    "  \n",
    "- Replace the LSTM module with a simple **feed-forward network (MLP)** and compare the performance.\n",
    "  \n",
    "- Replace the LSTM module with a standard **RNN** or a **stacked LSTM** architecture to evaluate whether more complex structures improve the results.\n",
    "  \n",
    "- Apply a normalization procedure (e.g., **min-max scaling**) to the data, ensuring that only the training data is used for fitting the scaler. Perform the modeling process on the normalized data and, after generating the final model's predictions, invert the normalization to return the output to its original scale. See `sklearn.preprocessing.MinMaxScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))\n",
    "\n",
    "- Additionally, perform the modeling on the **raw data**, without applying any transformation (such as converting it into log daily returns), to compare results directly with the untransformed dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- To understand the limitations of LSTM in time series modeling, proceed to Notebook 5.1, where we will explore and implement the LSTNet architecture.\n",
    "\n",
    "- To learn about more advanced deep learning based approaches, proceed to module 6 (Transformer based models) or module 7 (LLM-based models).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chains1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
